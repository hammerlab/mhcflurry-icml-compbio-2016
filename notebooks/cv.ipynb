{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = 'cuda.root=/usr/local/cuda,floatX=float32,device=gpu3,force_device=False'\n",
    "\n",
    "import theano\n",
    "print(theano.config.device)\n",
    "\n",
    "import mhcflurry, seaborn, numpy, pandas, pickle, sklearn, collections, scipy, time, logging, sys\n",
    "import mhcflurry.dataset\n",
    "import fancyimpute\n",
    "\n",
    "import sklearn.metrics\n",
    "import sklearn.cross_validation\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pepdata\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_peptides_to_consider_allele = 10\n",
    "max_ic50 = 50000\n",
    "data_dir = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_train_data = mhcflurry.dataset.Dataset.from_csv(data_dir + \"bdata.2009.mhci.public.1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>allele</th>\n",
       "      <th>peptide_length</th>\n",
       "      <th>cv</th>\n",
       "      <th>peptide</th>\n",
       "      <th>inequality</th>\n",
       "      <th>affinity</th>\n",
       "      <th>sample_weight</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allele</th>\n",
       "      <th>peptide</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"14\" valign=\"top\">ELA-A1</th>\n",
       "      <th>GSQKLTTGNCNW</th>\n",
       "      <td>None</td>\n",
       "      <td>ELA-A1</td>\n",
       "      <td>12</td>\n",
       "      <td>TBD</td>\n",
       "      <td>GSQKLTTGNCNW</td>\n",
       "      <td>=</td>\n",
       "      <td>605.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HVKDETNTTEYW</th>\n",
       "      <td>None</td>\n",
       "      <td>ELA-A1</td>\n",
       "      <td>12</td>\n",
       "      <td>TBD</td>\n",
       "      <td>HVKDETNTTEYW</td>\n",
       "      <td>=</td>\n",
       "      <td>880.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LVEDVTNTAEYW</th>\n",
       "      <td>None</td>\n",
       "      <td>ELA-A1</td>\n",
       "      <td>12</td>\n",
       "      <td>TBD</td>\n",
       "      <td>LVEDVTNTAEYW</td>\n",
       "      <td>=</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RVEDKTNTAEYW</th>\n",
       "      <td>None</td>\n",
       "      <td>ELA-A1</td>\n",
       "      <td>12</td>\n",
       "      <td>TBD</td>\n",
       "      <td>RVEDKTNTAEYW</td>\n",
       "      <td>=</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RVEDVKNTAEYW</th>\n",
       "      <td>None</td>\n",
       "      <td>ELA-A1</td>\n",
       "      <td>12</td>\n",
       "      <td>TBD</td>\n",
       "      <td>RVEDVKNTAEYW</td>\n",
       "      <td>=</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RVEDVTLTAEYW</th>\n",
       "      <td>None</td>\n",
       "      <td>ELA-A1</td>\n",
       "      <td>12</td>\n",
       "      <td>TBD</td>\n",
       "      <td>RVEDVTLTAEYW</td>\n",
       "      <td>=</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RVEDVTNKAEYW</th>\n",
       "      <td>None</td>\n",
       "      <td>ELA-A1</td>\n",
       "      <td>12</td>\n",
       "      <td>TBD</td>\n",
       "      <td>RVEDVTNKAEYW</td>\n",
       "      <td>=</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RVEDVTNTAELW</th>\n",
       "      <td>None</td>\n",
       "      <td>ELA-A1</td>\n",
       "      <td>12</td>\n",
       "      <td>TBD</td>\n",
       "      <td>RVEDVTNTAELW</td>\n",
       "      <td>=</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RVEDVTNTAEYL</th>\n",
       "      <td>None</td>\n",
       "      <td>ELA-A1</td>\n",
       "      <td>12</td>\n",
       "      <td>TBD</td>\n",
       "      <td>RVEDVTNTAEYL</td>\n",
       "      <td>=</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RVEDVTNTAEYW</th>\n",
       "      <td>None</td>\n",
       "      <td>ELA-A1</td>\n",
       "      <td>12</td>\n",
       "      <td>TBD</td>\n",
       "      <td>RVEDVTNTAEYW</td>\n",
       "      <td>=</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RVEDVTNTALYW</th>\n",
       "      <td>None</td>\n",
       "      <td>ELA-A1</td>\n",
       "      <td>12</td>\n",
       "      <td>TBD</td>\n",
       "      <td>RVEDVTNTALYW</td>\n",
       "      <td>=</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RVEDVTNTKEYW</th>\n",
       "      <td>None</td>\n",
       "      <td>ELA-A1</td>\n",
       "      <td>12</td>\n",
       "      <td>TBD</td>\n",
       "      <td>RVEDVTNTKEYW</td>\n",
       "      <td>=</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RVEKVTNTAEYW</th>\n",
       "      <td>None</td>\n",
       "      <td>ELA-A1</td>\n",
       "      <td>12</td>\n",
       "      <td>TBD</td>\n",
       "      <td>RVEKVTNTAEYW</td>\n",
       "      <td>=</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RVLDVTNTAEYW</th>\n",
       "      <td>None</td>\n",
       "      <td>ELA-A1</td>\n",
       "      <td>12</td>\n",
       "      <td>TBD</td>\n",
       "      <td>RVLDVTNTAEYW</td>\n",
       "      <td>=</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"15\" valign=\"top\">Gogo-B0101</th>\n",
       "      <th>RRFVNVVPTF</th>\n",
       "      <td>gorilla</td>\n",
       "      <td>Gogo-B0101</td>\n",
       "      <td>10</td>\n",
       "      <td>TBD</td>\n",
       "      <td>RRFVNVVPTF</td>\n",
       "      <td>=</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ERYLKDQQL</th>\n",
       "      <td>gorilla</td>\n",
       "      <td>Gogo-B0101</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>ERYLKDQQL</td>\n",
       "      <td>=</td>\n",
       "      <td>14579.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRFKLIVLY</th>\n",
       "      <td>gorilla</td>\n",
       "      <td>Gogo-B0101</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>GRFKLIVLY</td>\n",
       "      <td>=</td>\n",
       "      <td>3091.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IDFPKTFGW</th>\n",
       "      <td>gorilla</td>\n",
       "      <td>Gogo-B0101</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>IDFPKTFGW</td>\n",
       "      <td>=</td>\n",
       "      <td>107383.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IFFPKTFGW</th>\n",
       "      <td>gorilla</td>\n",
       "      <td>Gogo-B0101</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>IFFPKTFGW</td>\n",
       "      <td>=</td>\n",
       "      <td>3174.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IKFPKTFGW</th>\n",
       "      <td>gorilla</td>\n",
       "      <td>Gogo-B0101</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>IKFPKTFGW</td>\n",
       "      <td>=</td>\n",
       "      <td>3274.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ILFPKTFGW</th>\n",
       "      <td>gorilla</td>\n",
       "      <td>Gogo-B0101</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>ILFPKTFGW</td>\n",
       "      <td>=</td>\n",
       "      <td>998.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INFPKTFGW</th>\n",
       "      <td>gorilla</td>\n",
       "      <td>Gogo-B0101</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>INFPKTFGW</td>\n",
       "      <td>=</td>\n",
       "      <td>23443.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IRFPKTFGW</th>\n",
       "      <td>gorilla</td>\n",
       "      <td>Gogo-B0101</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>IRFPKTFGW</td>\n",
       "      <td>=</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IRYPKTFGW</th>\n",
       "      <td>gorilla</td>\n",
       "      <td>Gogo-B0101</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>IRYPKTFGW</td>\n",
       "      <td>=</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KRGILTLKY</th>\n",
       "      <td>gorilla</td>\n",
       "      <td>Gogo-B0101</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>KRGILTLKY</td>\n",
       "      <td>=</td>\n",
       "      <td>668.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KRKKAYADF</th>\n",
       "      <td>gorilla</td>\n",
       "      <td>Gogo-B0101</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>KRKKAYADF</td>\n",
       "      <td>=</td>\n",
       "      <td>6115.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KRYKSIVKY</th>\n",
       "      <td>gorilla</td>\n",
       "      <td>Gogo-B0101</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>KRYKSIVKY</td>\n",
       "      <td>=</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RRYQKSTEL</th>\n",
       "      <td>gorilla</td>\n",
       "      <td>Gogo-B0101</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>RRYQKSTEL</td>\n",
       "      <td>=</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRDKTIIMW</th>\n",
       "      <td>gorilla</td>\n",
       "      <td>Gogo-B0101</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>SRDKTIIMW</td>\n",
       "      <td>=</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-2-DB</th>\n",
       "      <th>AAAAAAYAAM</th>\n",
       "      <td>mouse</td>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>10</td>\n",
       "      <td>TBD</td>\n",
       "      <td>AAAAAAYAAM</td>\n",
       "      <td>=</td>\n",
       "      <td>7333.333333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">Patr-B2401</th>\n",
       "      <th>SLYLELDTI</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>SLYLELDTI</td>\n",
       "      <td>=</td>\n",
       "      <td>22409.304697</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SNYLELDTI</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>SNYLELDTI</td>\n",
       "      <td>=</td>\n",
       "      <td>4803.932379</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPYLELDTI</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>SPYLELDTI</td>\n",
       "      <td>=</td>\n",
       "      <td>8210.634469</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SQYLELDTI</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>SQYLELDTI</td>\n",
       "      <td>=</td>\n",
       "      <td>3841.245189</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRYLELDTI</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>SRYLELDTI</td>\n",
       "      <td>=</td>\n",
       "      <td>18216.870451</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SSYLELDTI</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>SSYLELDTI</td>\n",
       "      <td>=</td>\n",
       "      <td>12389.030549</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STYLELDTI</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>STYLELDTI</td>\n",
       "      <td>=</td>\n",
       "      <td>22995.859278</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVYLELDTI</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>SVYLELDTI</td>\n",
       "      <td>=</td>\n",
       "      <td>1504.790451</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SYYLELDTI</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>SYYLELDTI</td>\n",
       "      <td>=</td>\n",
       "      <td>21620.468423</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDATSILGI</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>TDATSILGI</td>\n",
       "      <td>=</td>\n",
       "      <td>9.565772</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDNSSPPAV</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>TDNSSPPAV</td>\n",
       "      <td>=</td>\n",
       "      <td>69.913924</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDYLELDTI</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>TDYLELDTI</td>\n",
       "      <td>=</td>\n",
       "      <td>12.150058</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TEAMTRYSA</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>TEAMTRYSA</td>\n",
       "      <td>=</td>\n",
       "      <td>36866.750907</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TESTLSTAL</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>TESTLSTAL</td>\n",
       "      <td>=</td>\n",
       "      <td>52196.730666</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TILGIGTVL</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>TILGIGTVL</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>78125.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VDFIPVENL</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>VDFIPVENL</td>\n",
       "      <td>=</td>\n",
       "      <td>1304.671425</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VDILAGYGA</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>VDILAGYGA</td>\n",
       "      <td>=</td>\n",
       "      <td>1529.550065</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VDKNPHNTA</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>VDKNPHNTA</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>78125.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VDPNIRTGV</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>VDPNIRTGV</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>78125.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VDVQYLYGV</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>VDVQYLYGV</td>\n",
       "      <td>=</td>\n",
       "      <td>26.168385</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VDYLELDTI</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>VDYLELDTI</td>\n",
       "      <td>=</td>\n",
       "      <td>8.751466</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VEAQLHVWV</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>VEAQLHVWV</td>\n",
       "      <td>=</td>\n",
       "      <td>11679.657915</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VESENKVVI</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>VESENKVVI</td>\n",
       "      <td>=</td>\n",
       "      <td>12912.904034</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WDQMWKCLI</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>WDQMWKCLI</td>\n",
       "      <td>=</td>\n",
       "      <td>45.677950</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WEQDLQHGA</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>WEQDLQHGA</td>\n",
       "      <td>=</td>\n",
       "      <td>58495.538057</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WETARHTPV</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>WETARHTPV</td>\n",
       "      <td>=</td>\n",
       "      <td>21366.204458</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WEYVVLLFL</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>WEYVVLLFL</td>\n",
       "      <td>=</td>\n",
       "      <td>3422.601957</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YAAQGYKVL</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>YAAQGYKVL</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>78125.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YDVVSKLPL</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>YDVVSKLPL</td>\n",
       "      <td>=</td>\n",
       "      <td>281.737403</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YVQMALMKL</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B2401</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>YVQMALMKL</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>78125.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137654 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            species      allele  peptide_length   cv  \\\n",
       "allele     peptide                                                     \n",
       "ELA-A1     GSQKLTTGNCNW        None      ELA-A1              12  TBD   \n",
       "           HVKDETNTTEYW        None      ELA-A1              12  TBD   \n",
       "           LVEDVTNTAEYW        None      ELA-A1              12  TBD   \n",
       "           RVEDKTNTAEYW        None      ELA-A1              12  TBD   \n",
       "           RVEDVKNTAEYW        None      ELA-A1              12  TBD   \n",
       "           RVEDVTLTAEYW        None      ELA-A1              12  TBD   \n",
       "           RVEDVTNKAEYW        None      ELA-A1              12  TBD   \n",
       "           RVEDVTNTAELW        None      ELA-A1              12  TBD   \n",
       "           RVEDVTNTAEYL        None      ELA-A1              12  TBD   \n",
       "           RVEDVTNTAEYW        None      ELA-A1              12  TBD   \n",
       "           RVEDVTNTALYW        None      ELA-A1              12  TBD   \n",
       "           RVEDVTNTKEYW        None      ELA-A1              12  TBD   \n",
       "           RVEKVTNTAEYW        None      ELA-A1              12  TBD   \n",
       "           RVLDVTNTAEYW        None      ELA-A1              12  TBD   \n",
       "Gogo-B0101 RRFVNVVPTF       gorilla  Gogo-B0101              10  TBD   \n",
       "           ERYLKDQQL        gorilla  Gogo-B0101               9  TBD   \n",
       "           GRFKLIVLY        gorilla  Gogo-B0101               9  TBD   \n",
       "           IDFPKTFGW        gorilla  Gogo-B0101               9  TBD   \n",
       "           IFFPKTFGW        gorilla  Gogo-B0101               9  TBD   \n",
       "           IKFPKTFGW        gorilla  Gogo-B0101               9  TBD   \n",
       "           ILFPKTFGW        gorilla  Gogo-B0101               9  TBD   \n",
       "           INFPKTFGW        gorilla  Gogo-B0101               9  TBD   \n",
       "           IRFPKTFGW        gorilla  Gogo-B0101               9  TBD   \n",
       "           IRYPKTFGW        gorilla  Gogo-B0101               9  TBD   \n",
       "           KRGILTLKY        gorilla  Gogo-B0101               9  TBD   \n",
       "           KRKKAYADF        gorilla  Gogo-B0101               9  TBD   \n",
       "           KRYKSIVKY        gorilla  Gogo-B0101               9  TBD   \n",
       "           RRYQKSTEL        gorilla  Gogo-B0101               9  TBD   \n",
       "           SRDKTIIMW        gorilla  Gogo-B0101               9  TBD   \n",
       "H-2-DB     AAAAAAYAAM         mouse      H-2-DB              10  TBD   \n",
       "...                             ...         ...             ...  ...   \n",
       "Patr-B2401 SLYLELDTI     chimpanzee  Patr-B2401               9  TBD   \n",
       "           SNYLELDTI     chimpanzee  Patr-B2401               9  TBD   \n",
       "           SPYLELDTI     chimpanzee  Patr-B2401               9  TBD   \n",
       "           SQYLELDTI     chimpanzee  Patr-B2401               9  TBD   \n",
       "           SRYLELDTI     chimpanzee  Patr-B2401               9  TBD   \n",
       "           SSYLELDTI     chimpanzee  Patr-B2401               9  TBD   \n",
       "           STYLELDTI     chimpanzee  Patr-B2401               9  TBD   \n",
       "           SVYLELDTI     chimpanzee  Patr-B2401               9  TBD   \n",
       "           SYYLELDTI     chimpanzee  Patr-B2401               9  TBD   \n",
       "           TDATSILGI     chimpanzee  Patr-B2401               9  TBD   \n",
       "           TDNSSPPAV     chimpanzee  Patr-B2401               9  TBD   \n",
       "           TDYLELDTI     chimpanzee  Patr-B2401               9  TBD   \n",
       "           TEAMTRYSA     chimpanzee  Patr-B2401               9  TBD   \n",
       "           TESTLSTAL     chimpanzee  Patr-B2401               9  TBD   \n",
       "           TILGIGTVL     chimpanzee  Patr-B2401               9  TBD   \n",
       "           VDFIPVENL     chimpanzee  Patr-B2401               9  TBD   \n",
       "           VDILAGYGA     chimpanzee  Patr-B2401               9  TBD   \n",
       "           VDKNPHNTA     chimpanzee  Patr-B2401               9  TBD   \n",
       "           VDPNIRTGV     chimpanzee  Patr-B2401               9  TBD   \n",
       "           VDVQYLYGV     chimpanzee  Patr-B2401               9  TBD   \n",
       "           VDYLELDTI     chimpanzee  Patr-B2401               9  TBD   \n",
       "           VEAQLHVWV     chimpanzee  Patr-B2401               9  TBD   \n",
       "           VESENKVVI     chimpanzee  Patr-B2401               9  TBD   \n",
       "           WDQMWKCLI     chimpanzee  Patr-B2401               9  TBD   \n",
       "           WEQDLQHGA     chimpanzee  Patr-B2401               9  TBD   \n",
       "           WETARHTPV     chimpanzee  Patr-B2401               9  TBD   \n",
       "           WEYVVLLFL     chimpanzee  Patr-B2401               9  TBD   \n",
       "           YAAQGYKVL     chimpanzee  Patr-B2401               9  TBD   \n",
       "           YDVVSKLPL     chimpanzee  Patr-B2401               9  TBD   \n",
       "           YVQMALMKL     chimpanzee  Patr-B2401               9  TBD   \n",
       "\n",
       "                              peptide inequality       affinity  sample_weight  \n",
       "allele     peptide                                                              \n",
       "ELA-A1     GSQKLTTGNCNW  GSQKLTTGNCNW          =     605.000000            1.0  \n",
       "           HVKDETNTTEYW  HVKDETNTTEYW          =     880.000000            1.0  \n",
       "           LVEDVTNTAEYW  LVEDVTNTAEYW          =     170.000000            1.0  \n",
       "           RVEDKTNTAEYW  RVEDKTNTAEYW          =      70.000000            1.0  \n",
       "           RVEDVKNTAEYW  RVEDVKNTAEYW          =      65.000000            1.0  \n",
       "           RVEDVTLTAEYW  RVEDVTLTAEYW          =     150.000000            1.0  \n",
       "           RVEDVTNKAEYW  RVEDVTNKAEYW          =      80.000000            1.0  \n",
       "           RVEDVTNTAELW  RVEDVTNTAELW          =      25.000000            1.0  \n",
       "           RVEDVTNTAEYL  RVEDVTNTAEYL          =      97.000000            1.0  \n",
       "           RVEDVTNTAEYW  RVEDVTNTAEYW          =      39.000000            1.0  \n",
       "           RVEDVTNTALYW  RVEDVTNTALYW          =      78.000000            1.0  \n",
       "           RVEDVTNTKEYW  RVEDVTNTKEYW          =      36.000000            1.0  \n",
       "           RVEKVTNTAEYW  RVEKVTNTAEYW          =     110.000000            1.0  \n",
       "           RVLDVTNTAEYW  RVLDVTNTAEYW          =     520.000000            1.0  \n",
       "Gogo-B0101 RRFVNVVPTF      RRFVNVVPTF          =     196.000000            1.0  \n",
       "           ERYLKDQQL        ERYLKDQQL          =   14579.000000            1.0  \n",
       "           GRFKLIVLY        GRFKLIVLY          =    3091.000000            1.0  \n",
       "           IDFPKTFGW        IDFPKTFGW          =  107383.000000            1.0  \n",
       "           IFFPKTFGW        IFFPKTFGW          =    3174.000000            1.0  \n",
       "           IKFPKTFGW        IKFPKTFGW          =    3274.000000            1.0  \n",
       "           ILFPKTFGW        ILFPKTFGW          =     998.000000            1.0  \n",
       "           INFPKTFGW        INFPKTFGW          =   23443.000000            1.0  \n",
       "           IRFPKTFGW        IRFPKTFGW          =      30.000000            1.0  \n",
       "           IRYPKTFGW        IRYPKTFGW          =      54.000000            1.0  \n",
       "           KRGILTLKY        KRGILTLKY          =     668.000000            1.0  \n",
       "           KRKKAYADF        KRKKAYADF          =    6115.000000            1.0  \n",
       "           KRYKSIVKY        KRYKSIVKY          =     100.000000            1.0  \n",
       "           RRYQKSTEL        RRYQKSTEL          =      53.000000            1.0  \n",
       "           SRDKTIIMW        SRDKTIIMW          =     128.000000            1.0  \n",
       "H-2-DB     AAAAAAYAAM      AAAAAAYAAM          =    7333.333333            1.0  \n",
       "...                               ...        ...            ...            ...  \n",
       "Patr-B2401 SLYLELDTI        SLYLELDTI          =   22409.304697            1.0  \n",
       "           SNYLELDTI        SNYLELDTI          =    4803.932379            1.0  \n",
       "           SPYLELDTI        SPYLELDTI          =    8210.634469            1.0  \n",
       "           SQYLELDTI        SQYLELDTI          =    3841.245189            1.0  \n",
       "           SRYLELDTI        SRYLELDTI          =   18216.870451            1.0  \n",
       "           SSYLELDTI        SSYLELDTI          =   12389.030549            1.0  \n",
       "           STYLELDTI        STYLELDTI          =   22995.859278            1.0  \n",
       "           SVYLELDTI        SVYLELDTI          =    1504.790451            1.0  \n",
       "           SYYLELDTI        SYYLELDTI          =   21620.468423            1.0  \n",
       "           TDATSILGI        TDATSILGI          =       9.565772            1.0  \n",
       "           TDNSSPPAV        TDNSSPPAV          =      69.913924            1.0  \n",
       "           TDYLELDTI        TDYLELDTI          =      12.150058            1.0  \n",
       "           TEAMTRYSA        TEAMTRYSA          =   36866.750907            1.0  \n",
       "           TESTLSTAL        TESTLSTAL          =   52196.730666            1.0  \n",
       "           TILGIGTVL        TILGIGTVL          >   78125.000000            1.0  \n",
       "           VDFIPVENL        VDFIPVENL          =    1304.671425            1.0  \n",
       "           VDILAGYGA        VDILAGYGA          =    1529.550065            1.0  \n",
       "           VDKNPHNTA        VDKNPHNTA          >   78125.000000            1.0  \n",
       "           VDPNIRTGV        VDPNIRTGV          >   78125.000000            1.0  \n",
       "           VDVQYLYGV        VDVQYLYGV          =      26.168385            1.0  \n",
       "           VDYLELDTI        VDYLELDTI          =       8.751466            1.0  \n",
       "           VEAQLHVWV        VEAQLHVWV          =   11679.657915            1.0  \n",
       "           VESENKVVI        VESENKVVI          =   12912.904034            1.0  \n",
       "           WDQMWKCLI        WDQMWKCLI          =      45.677950            1.0  \n",
       "           WEQDLQHGA        WEQDLQHGA          =   58495.538057            1.0  \n",
       "           WETARHTPV        WETARHTPV          =   21366.204458            1.0  \n",
       "           WEYVVLLFL        WEYVVLLFL          =    3422.601957            1.0  \n",
       "           YAAQGYKVL        YAAQGYKVL          >   78125.000000            1.0  \n",
       "           YDVVSKLPL        YDVVSKLPL          =     281.737403            1.0  \n",
       "           YVQMALMKL        YVQMALMKL          >   78125.000000            1.0  \n",
       "\n",
       "[137654 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_data._df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HLA-A2602    202\n",
       "HLA-A2603    205\n",
       "Name: allele, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alleles = [\n",
    "    #\"HLA-A0201\",\n",
    "    # \"HLA-A0301\",\n",
    "    # \"HLA-A0203\",\n",
    "    \"HLA-A2602\",\n",
    "    \"HLA-A2603\",\n",
    "    # 'HLA-B7301',\n",
    "]\n",
    "all_train_data._df.allele.value_counts()[alleles]\n",
    "#alleles = alleles[:1] + alleles[-1:]\n",
    "#alleles = [allele for allele in all_train_data if len(all_train_data[allele].Y) >= min_peptides_to_consider_allele]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_to_ic50(log_value):\n",
    "        \"\"\"\n",
    "        Convert neural network output to IC50 values between 0.0 and\n",
    "        self.max_ic50 (typically 5000, 20000 or 50000)\n",
    "        \"\"\"\n",
    "        return max_ic50 ** (1.0 - log_value)\n",
    "\n",
    "def make_scores(ic50_y, ic50_y_pred, sample_weight=None, threshold_nm=500):     \n",
    "    y_pred = mhcflurry.regression_target.ic50_to_regression_target(ic50_y_pred, max_ic50)\n",
    "    try:\n",
    "        auc = sklearn.metrics.roc_auc_score(ic50_y <= threshold_nm, y_pred, sample_weight=sample_weight)\n",
    "    except ValueError:\n",
    "        auc = numpy.nan\n",
    "    try:\n",
    "        f1 = sklearn.metrics.f1_score(ic50_y <= threshold_nm, ic50_y_pred <= threshold_nm, sample_weight=sample_weight)\n",
    "    except ValueError:\n",
    "        f1 = numpy.nan\n",
    "    try:\n",
    "        tau = scipy.stats.kendalltau(ic50_y_pred, ic50_y)[0]\n",
    "    except ValueError:\n",
    "        tau = numpy.nan\n",
    "    \n",
    "    return dict(\n",
    "        auc=auc,\n",
    "        f1=f1,\n",
    "        tau=tau,\n",
    "    )    \n",
    "\n",
    "    \n",
    "def Xmake_scores(y, y_pred, weights=None, sample_weight=None, threshold_nm=500):\n",
    "    ic50_y = log_to_ic50(y)\n",
    "    ic50_y_pred = log_to_ic50(y_pred) \n",
    "    return dict(\n",
    "        auc=sklearn.metrics.roc_auc_score(ic50_y <= threshold_nm, y_pred, sample_weight=sample_weight),\n",
    "        f1=sklearn.metrics.f1_score(ic50_y <= threshold_nm, ic50_y_pred <= threshold_nm, sample_weight=sample_weight),\n",
    "        tau=scipy.stats.kendalltau(y_pred, y)[0],\n",
    "    )    \n",
    "\n",
    "def mean_with_std(grouped_column, decimals=3):\n",
    "    pattern = \"%%0.%df\" % decimals\n",
    "    return pandas.Series([\n",
    "        (pattern + \" +/ \" + pattern) % (m, s) if not pandas.isnull(s) else pattern % m\n",
    "        for (m, s) in zip(grouped_column.mean(), grouped_column.std())\n",
    "    ], index = grouped_column.mean().index)\n",
    "\n",
    "\n",
    "\n",
    "def Xcollapse_9mer_affinities(Y_9mer_true, Y_9mer_pred, original_peptides):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_9mer_true : np.array of float\n",
    "        True regression target values for 9mers extracted from longer/shorter peptides\n",
    "    \n",
    "    Y_9mer_pred : np.array of float\n",
    "        Predicted values for 9mers\n",
    "    \n",
    "    original_peptides : np.array of str\n",
    "        Original peptides of varying length that 9mers were extracted from.\n",
    "    \"\"\"\n",
    "    # collapse multiple 9mer predictions and measured values into \n",
    "    # smaller set of predictions for peptides of varying lengths\n",
    "    peptide_to_true_affinity_dict = defaultdict(list)\n",
    "    peptide_to_predicted_affinity_dict = defaultdict(list)\n",
    "    for i, p in enumerate(original_peptides):\n",
    "        peptide_to_true_affinity_dict[p].append(Y_9mer_true[i])\n",
    "        peptide_to_predicted_affinity_dict[p].append(Y_9mer_pred[i])\n",
    "\n",
    "    unique_peptides = list(sorted(set(peptide_to_predicted_affinity_dict.keys())))\n",
    "    print(\"-- # unique peptides = %d\" % (len(unique_peptides),))\n",
    "    Y_true = np.array([\n",
    "            np.mean(peptide_to_true_affinity_dict[p]) for p in unique_peptides ])\n",
    "    Y_pred = np.array([\n",
    "            np.mean(peptide_to_predicted_affinity_dict[p]) for p in unique_peptides])\n",
    "    return Y_true, Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'activation',\n",
       " 'dropout_probability',\n",
       " 'embedding_output_dim',\n",
       " 'fraction_negative',\n",
       " 'impute',\n",
       " 'layer_sizes',\n",
       " 'n_training_epochs',\n",
       " 'pretrain_decay'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_probabilities = [0.5]\n",
    "\n",
    "#embedding_output_dims = [32]\n",
    "embedding_output_dims = [4, 32]\n",
    "\n",
    "#layer_sizes = [[4], [8], [16], [64], [128]]\n",
    "layer_sizes_list = [[4], [64], [128], [256]]\n",
    "\n",
    "activations = [\"tanh\"]\n",
    "\n",
    "models_params_list = []\n",
    "for n_training_epochs in [250, 500, 1000]:\n",
    "    for impute in [False, True]:\n",
    "        for pretrain_decay in [\"1 / (1+epoch)**2\"]: #\"numpy.exp(-epoch)\"]:\n",
    "            for fraction_negative in [.10]:\n",
    "                for dropout_probability in dropout_probabilities:\n",
    "                    for embedding_output_dim in embedding_output_dims:\n",
    "                        for layer_sizes in layer_sizes_list:\n",
    "                            for activation in activations:\n",
    "                                models_params_list.append(dict(\n",
    "                                    n_training_epochs=n_training_epochs,\n",
    "                                    impute=impute,\n",
    "                                    pretrain_decay=pretrain_decay,\n",
    "                                    fraction_negative=fraction_negative,\n",
    "                                    dropout_probability=dropout_probability,  \n",
    "                                    embedding_output_dim=embedding_output_dim,\n",
    "                                    layer_sizes=layer_sizes,\n",
    "                                    activation=activation))\n",
    "\n",
    "print(\"%d models\" % len(models_params_list))\n",
    "models_params_explored = set.union(*[set(x) for x in models_params_list])\n",
    "models_params_explored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'mhcflurry' from '/home/tim/sinai/git/mhcflurry/mhcflurry/__init__.pyc'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(mhcflurry.class1_binding_predictor)\n",
    "reload(mhcflurry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbmr4_transformer = pepdata.reduced_alphabet.make_alphabet_transformer(\"gbmr4\")\n",
    "def default_projector(peptide):\n",
    "    \"\"\"\n",
    "    Given a peptide, return a list of projections for it. The projections are:\n",
    "        - the gbmr4 reduced representation\n",
    "        - for all positions in the peptide, the peptide with a \".\" replacing the residue at that position\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    peptide : string\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    string list\n",
    "    \"\"\"\n",
    "    def projections(peptide, edit_distance=1):\n",
    "        if edit_distance == 0:\n",
    "            return set([peptide])\n",
    "        return set.union(*[\n",
    "                projections(p, edit_distance - 1)\n",
    "                for p in (peptide[0:i] + \".\" + peptide[(i+1):] for i in range(len(peptide)))\n",
    "        ])\n",
    "    return sorted(projections(peptide)) + [gbmr4_transformer(peptide)]\n",
    "\n",
    "def similar_peptides(set1, set2, projector=default_projector):\n",
    "    \"\"\"\n",
    "    Given two sets of peptides, return a list of the peptides whose reduced representations\n",
    "    are found in both sets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    projector : (string -> string) or (string -> string list)\n",
    "        Function giving projection(s) of a peptide\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    string list\n",
    "        \n",
    "    \"\"\"\n",
    "    result = collections.defaultdict(lambda: ([], []))\n",
    "    for (index, peptides) in enumerate([set1, set2]):\n",
    "        for peptide in peptides:\n",
    "            projections = projector(peptide)\n",
    "            if not isinstance(projections, list):\n",
    "                projections = [projections]\n",
    "            for projection in projections:\n",
    "                result[projection][index].append(peptide)\n",
    "    \n",
    "    common = set()\n",
    "    for (peptides1, peptides2) in result.values():\n",
    "        if peptides1 and peptides2:\n",
    "            common.update(peptides1 + peptides2)\n",
    "\n",
    "    return sorted(common)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imputer = fancyimpute.MICE(n_imputations=50, n_burn_in=5, n_nearest_columns=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allele: HLA-A2602\n",
      "Peptides to remove: 10: ['DILASIIDY', 'DTTTDISKY', 'EVIEQWHSL', 'KQWGWFALL', 'QVGIFLICK', 'REMGIVDLL', 'RRMATTFTF', 'RRYTRRISL', 'TTAKAMEQM', 'TVGYMYIMK']\n",
      "After dropping similar peptides, test size 68 -> 63\n",
      "Dropping 12246 peptides with <2 observations\n",
      "Dropping 9 alleles with <2 observations: ['ELA-A1', 'HLA-B2701', 'HLA-B3508', 'HLA-B44', 'HLA-E0101', 'Mamu-B04', 'Patr-A0602', 'Patr-B0901', 'Patr-B1701']\n",
      "[MICE] Completing matrix with shape (19292, 97)\n",
      "[MICE] Starting imputation round 1/110, elapsed time 0.020\n",
      "[MICE] Starting imputation round 2/110, elapsed time 1.569\n",
      "[MICE] Starting imputation round 3/110, elapsed time 3.111\n",
      "[MICE] Starting imputation round 4/110, elapsed time 4.665\n",
      "[MICE] Starting imputation round 5/110, elapsed time 6.208\n",
      "[MICE] Starting imputation round 6/110, elapsed time 7.737\n",
      "[MICE] Starting imputation round 7/110, elapsed time 9.265\n",
      "[MICE] Starting imputation round 8/110, elapsed time 10.791\n",
      "[MICE] Starting imputation round 9/110, elapsed time 12.317\n",
      "[MICE] Starting imputation round 10/110, elapsed time 13.843\n",
      "[MICE] Starting imputation round 11/110, elapsed time 15.252\n",
      "[MICE] Starting imputation round 12/110, elapsed time 16.644\n",
      "[MICE] Starting imputation round 13/110, elapsed time 18.050\n",
      "[MICE] Starting imputation round 14/110, elapsed time 19.488\n",
      "[MICE] Starting imputation round 15/110, elapsed time 20.925\n",
      "[MICE] Starting imputation round 16/110, elapsed time 22.360\n",
      "[MICE] Starting imputation round 17/110, elapsed time 23.760\n",
      "[MICE] Starting imputation round 18/110, elapsed time 25.169\n",
      "[MICE] Starting imputation round 19/110, elapsed time 26.581\n",
      "[MICE] Starting imputation round 20/110, elapsed time 28.087\n",
      "[MICE] Starting imputation round 21/110, elapsed time 29.626\n",
      "[MICE] Starting imputation round 22/110, elapsed time 31.160\n",
      "[MICE] Starting imputation round 23/110, elapsed time 32.700\n",
      "[MICE] Starting imputation round 24/110, elapsed time 34.274\n",
      "[MICE] Starting imputation round 25/110, elapsed time 35.836\n",
      "[MICE] Starting imputation round 26/110, elapsed time 37.390\n",
      "[MICE] Starting imputation round 27/110, elapsed time 38.946\n",
      "[MICE] Starting imputation round 28/110, elapsed time 40.448\n",
      "[MICE] Starting imputation round 29/110, elapsed time 41.820\n",
      "[MICE] Starting imputation round 30/110, elapsed time 43.348\n",
      "[MICE] Starting imputation round 31/110, elapsed time 44.899\n",
      "[MICE] Starting imputation round 32/110, elapsed time 46.454\n",
      "[MICE] Starting imputation round 33/110, elapsed time 48.042\n",
      "[MICE] Starting imputation round 34/110, elapsed time 49.600\n",
      "[MICE] Starting imputation round 35/110, elapsed time 51.143\n",
      "[MICE] Starting imputation round 36/110, elapsed time 52.683\n",
      "[MICE] Starting imputation round 37/110, elapsed time 54.228\n",
      "[MICE] Starting imputation round 38/110, elapsed time 55.784\n",
      "[MICE] Starting imputation round 39/110, elapsed time 57.333\n",
      "[MICE] Starting imputation round 40/110, elapsed time 58.878\n",
      "[MICE] Starting imputation round 41/110, elapsed time 60.430\n",
      "[MICE] Starting imputation round 42/110, elapsed time 61.982\n",
      "[MICE] Starting imputation round 43/110, elapsed time 63.539\n",
      "[MICE] Starting imputation round 44/110, elapsed time 65.008\n",
      "[MICE] Starting imputation round 45/110, elapsed time 66.472\n",
      "[MICE] Starting imputation round 46/110, elapsed time 68.019\n",
      "[MICE] Starting imputation round 47/110, elapsed time 69.465\n",
      "[MICE] Starting imputation round 48/110, elapsed time 71.006\n",
      "[MICE] Starting imputation round 49/110, elapsed time 72.561\n",
      "[MICE] Starting imputation round 50/110, elapsed time 74.114\n",
      "[MICE] Starting imputation round 51/110, elapsed time 75.692\n",
      "[MICE] Starting imputation round 52/110, elapsed time 77.242\n",
      "[MICE] Starting imputation round 53/110, elapsed time 78.812\n",
      "[MICE] Starting imputation round 54/110, elapsed time 80.361\n",
      "[MICE] Starting imputation round 55/110, elapsed time 81.914\n",
      "[MICE] Starting imputation round 56/110, elapsed time 83.456\n",
      "[MICE] Starting imputation round 57/110, elapsed time 85.003\n",
      "[MICE] Starting imputation round 58/110, elapsed time 86.564\n",
      "[MICE] Starting imputation round 59/110, elapsed time 88.157\n",
      "[MICE] Starting imputation round 60/110, elapsed time 89.718\n",
      "[MICE] Starting imputation round 61/110, elapsed time 91.280\n",
      "[MICE] Starting imputation round 62/110, elapsed time 92.858\n",
      "[MICE] Starting imputation round 63/110, elapsed time 94.432\n",
      "[MICE] Starting imputation round 64/110, elapsed time 95.910\n",
      "[MICE] Starting imputation round 65/110, elapsed time 97.370\n",
      "[MICE] Starting imputation round 66/110, elapsed time 98.819\n",
      "[MICE] Starting imputation round 67/110, elapsed time 100.261\n",
      "[MICE] Starting imputation round 68/110, elapsed time 101.679\n",
      "[MICE] Starting imputation round 69/110, elapsed time 103.194\n",
      "[MICE] Starting imputation round 70/110, elapsed time 104.675\n",
      "[MICE] Starting imputation round 71/110, elapsed time 106.137\n",
      "[MICE] Starting imputation round 72/110, elapsed time 107.682\n",
      "[MICE] Starting imputation round 73/110, elapsed time 109.154\n",
      "[MICE] Starting imputation round 74/110, elapsed time 110.623\n",
      "[MICE] Starting imputation round 75/110, elapsed time 112.068\n",
      "[MICE] Starting imputation round 76/110, elapsed time 113.518\n",
      "[MICE] Starting imputation round 77/110, elapsed time 114.983\n",
      "[MICE] Starting imputation round 78/110, elapsed time 116.484\n",
      "[MICE] Starting imputation round 79/110, elapsed time 117.948\n",
      "[MICE] Starting imputation round 80/110, elapsed time 119.487\n",
      "[MICE] Starting imputation round 81/110, elapsed time 121.059\n",
      "[MICE] Starting imputation round 82/110, elapsed time 122.632\n",
      "[MICE] Starting imputation round 83/110, elapsed time 124.111\n",
      "[MICE] Starting imputation round 84/110, elapsed time 125.590\n",
      "[MICE] Starting imputation round 85/110, elapsed time 127.040\n",
      "[MICE] Starting imputation round 86/110, elapsed time 128.590\n",
      "[MICE] Starting imputation round 87/110, elapsed time 130.077\n",
      "[MICE] Starting imputation round 88/110, elapsed time 131.660\n",
      "[MICE] Starting imputation round 89/110, elapsed time 133.218\n",
      "[MICE] Starting imputation round 90/110, elapsed time 134.761\n",
      "[MICE] Starting imputation round 91/110, elapsed time 136.337\n",
      "[MICE] Starting imputation round 92/110, elapsed time 137.901\n",
      "[MICE] Starting imputation round 93/110, elapsed time 139.448\n",
      "[MICE] Starting imputation round 94/110, elapsed time 141.018\n",
      "[MICE] Starting imputation round 95/110, elapsed time 142.568\n",
      "[MICE] Starting imputation round 96/110, elapsed time 144.117\n",
      "[MICE] Starting imputation round 97/110, elapsed time 145.661\n",
      "[MICE] Starting imputation round 98/110, elapsed time 147.164\n",
      "[MICE] Starting imputation round 99/110, elapsed time 148.618\n",
      "[MICE] Starting imputation round 100/110, elapsed time 150.091\n",
      "[MICE] Starting imputation round 101/110, elapsed time 151.538\n",
      "[MICE] Starting imputation round 102/110, elapsed time 153.009\n",
      "[MICE] Starting imputation round 103/110, elapsed time 154.480\n",
      "[MICE] Starting imputation round 104/110, elapsed time 155.879\n",
      "[MICE] Starting imputation round 105/110, elapsed time 157.324\n",
      "[MICE] Starting imputation round 106/110, elapsed time 158.763\n",
      "[MICE] Starting imputation round 107/110, elapsed time 160.274\n",
      "[MICE] Starting imputation round 108/110, elapsed time 161.796\n",
      "[MICE] Starting imputation round 109/110, elapsed time 163.313\n",
      "[MICE] Starting imputation round 110/110, elapsed time 164.863\n",
      "Peptides to remove: 10: ['DILASIIDY', 'DTTTDISKY', 'EVADRVIFM', 'EVAESVMFM', 'EVIEQWHSL', 'QVGIFLICK', 'RRMATTFTF', 'RRYTRRISL', 'TTAKAMEQM', 'TVGYMYIMK']\n",
      "After dropping similar peptides, test size 67 -> 62\n",
      "Dropping 12240 peptides with <2 observations\n",
      "Dropping 9 alleles with <2 observations: ['ELA-A1', 'HLA-B2701', 'HLA-B3508', 'HLA-B44', 'HLA-E0101', 'Mamu-B04', 'Patr-A0602', 'Patr-B0901', 'Patr-B1701']\n",
      "[MICE] Completing matrix with shape (19298, 97)\n",
      "[MICE] Starting imputation round 1/110, elapsed time 0.020\n",
      "[MICE] Starting imputation round 2/110, elapsed time 1.649\n",
      "[MICE] Starting imputation round 3/110, elapsed time 3.280\n",
      "[MICE] Starting imputation round 4/110, elapsed time 4.918\n",
      "[MICE] Starting imputation round 5/110, elapsed time 6.547\n",
      "[MICE] Starting imputation round 6/110, elapsed time 8.398\n",
      "[MICE] Starting imputation round 7/110, elapsed time 10.967\n",
      "[MICE] Starting imputation round 8/110, elapsed time 13.556\n",
      "[MICE] Starting imputation round 9/110, elapsed time 16.025\n",
      "[MICE] Starting imputation round 10/110, elapsed time 17.582\n",
      "[MICE] Starting imputation round 11/110, elapsed time 19.025\n",
      "[MICE] Starting imputation round 12/110, elapsed time 20.477\n",
      "[MICE] Starting imputation round 13/110, elapsed time 21.930\n",
      "[MICE] Starting imputation round 14/110, elapsed time 23.382\n",
      "[MICE] Starting imputation round 15/110, elapsed time 24.829\n",
      "[MICE] Starting imputation round 16/110, elapsed time 26.279\n",
      "[MICE] Starting imputation round 17/110, elapsed time 27.732\n",
      "[MICE] Starting imputation round 18/110, elapsed time 29.180\n",
      "[MICE] Starting imputation round 19/110, elapsed time 30.627\n",
      "[MICE] Starting imputation round 20/110, elapsed time 32.091\n",
      "[MICE] Starting imputation round 21/110, elapsed time 33.548\n",
      "[MICE] Starting imputation round 22/110, elapsed time 35.001\n",
      "[MICE] Starting imputation round 23/110, elapsed time 36.459\n",
      "[MICE] Starting imputation round 24/110, elapsed time 37.918\n",
      "[MICE] Starting imputation round 25/110, elapsed time 39.372\n",
      "[MICE] Starting imputation round 26/110, elapsed time 40.831\n",
      "[MICE] Starting imputation round 27/110, elapsed time 42.290\n",
      "[MICE] Starting imputation round 28/110, elapsed time 43.744\n",
      "[MICE] Starting imputation round 29/110, elapsed time 45.205\n",
      "[MICE] Starting imputation round 30/110, elapsed time 46.660\n",
      "[MICE] Starting imputation round 31/110, elapsed time 48.117\n",
      "[MICE] Starting imputation round 32/110, elapsed time 49.574\n",
      "[MICE] Starting imputation round 33/110, elapsed time 51.033\n",
      "[MICE] Starting imputation round 34/110, elapsed time 52.497\n",
      "[MICE] Starting imputation round 35/110, elapsed time 53.962\n",
      "[MICE] Starting imputation round 36/110, elapsed time 55.416\n",
      "[MICE] Starting imputation round 37/110, elapsed time 56.871\n",
      "[MICE] Starting imputation round 38/110, elapsed time 58.332\n",
      "[MICE] Starting imputation round 39/110, elapsed time 59.789\n",
      "[MICE] Starting imputation round 40/110, elapsed time 61.244\n",
      "[MICE] Starting imputation round 41/110, elapsed time 62.714\n",
      "[MICE] Starting imputation round 42/110, elapsed time 64.184\n",
      "[MICE] Starting imputation round 43/110, elapsed time 65.652\n",
      "[MICE] Starting imputation round 44/110, elapsed time 67.114\n",
      "[MICE] Starting imputation round 45/110, elapsed time 68.578\n",
      "[MICE] Starting imputation round 46/110, elapsed time 70.044\n",
      "[MICE] Starting imputation round 47/110, elapsed time 71.505\n",
      "[MICE] Starting imputation round 48/110, elapsed time 72.960\n",
      "[MICE] Starting imputation round 49/110, elapsed time 74.417\n",
      "[MICE] Starting imputation round 50/110, elapsed time 75.879\n",
      "[MICE] Starting imputation round 51/110, elapsed time 77.338\n",
      "[MICE] Starting imputation round 52/110, elapsed time 78.791\n",
      "[MICE] Starting imputation round 53/110, elapsed time 80.243\n",
      "[MICE] Starting imputation round 54/110, elapsed time 81.707\n",
      "[MICE] Starting imputation round 55/110, elapsed time 83.163\n",
      "[MICE] Starting imputation round 56/110, elapsed time 84.623\n",
      "[MICE] Starting imputation round 57/110, elapsed time 86.072\n",
      "[MICE] Starting imputation round 58/110, elapsed time 87.529\n",
      "[MICE] Starting imputation round 59/110, elapsed time 88.981\n",
      "[MICE] Starting imputation round 60/110, elapsed time 90.437\n",
      "[MICE] Starting imputation round 61/110, elapsed time 91.889\n",
      "[MICE] Starting imputation round 62/110, elapsed time 93.354\n",
      "[MICE] Starting imputation round 63/110, elapsed time 94.806\n",
      "[MICE] Starting imputation round 64/110, elapsed time 96.261\n",
      "[MICE] Starting imputation round 65/110, elapsed time 97.715\n",
      "[MICE] Starting imputation round 66/110, elapsed time 99.176\n",
      "[MICE] Starting imputation round 67/110, elapsed time 100.632\n",
      "[MICE] Starting imputation round 68/110, elapsed time 102.085\n",
      "[MICE] Starting imputation round 69/110, elapsed time 103.534\n",
      "[MICE] Starting imputation round 70/110, elapsed time 104.989\n",
      "[MICE] Starting imputation round 71/110, elapsed time 106.443\n",
      "[MICE] Starting imputation round 72/110, elapsed time 107.899\n",
      "[MICE] Starting imputation round 73/110, elapsed time 109.365\n",
      "[MICE] Starting imputation round 74/110, elapsed time 110.830\n",
      "[MICE] Starting imputation round 75/110, elapsed time 112.275\n",
      "[MICE] Starting imputation round 76/110, elapsed time 113.735\n",
      "[MICE] Starting imputation round 77/110, elapsed time 115.187\n",
      "[MICE] Starting imputation round 78/110, elapsed time 116.638\n",
      "[MICE] Starting imputation round 79/110, elapsed time 118.088\n",
      "[MICE] Starting imputation round 80/110, elapsed time 119.545\n",
      "[MICE] Starting imputation round 81/110, elapsed time 121.016\n",
      "[MICE] Starting imputation round 82/110, elapsed time 122.486\n",
      "[MICE] Starting imputation round 83/110, elapsed time 123.947\n",
      "[MICE] Starting imputation round 84/110, elapsed time 125.591\n",
      "[MICE] Starting imputation round 85/110, elapsed time 127.058\n",
      "[MICE] Starting imputation round 86/110, elapsed time 128.512\n",
      "[MICE] Starting imputation round 87/110, elapsed time 130.032\n",
      "[MICE] Starting imputation round 88/110, elapsed time 131.661\n",
      "[MICE] Starting imputation round 89/110, elapsed time 133.295\n",
      "[MICE] Starting imputation round 90/110, elapsed time 134.920\n",
      "[MICE] Starting imputation round 91/110, elapsed time 136.412\n",
      "[MICE] Starting imputation round 92/110, elapsed time 137.919\n",
      "[MICE] Starting imputation round 93/110, elapsed time 139.547\n",
      "[MICE] Starting imputation round 94/110, elapsed time 141.180\n",
      "[MICE] Starting imputation round 95/110, elapsed time 142.791\n",
      "[MICE] Starting imputation round 96/110, elapsed time 144.342\n",
      "[MICE] Starting imputation round 97/110, elapsed time 145.954\n",
      "[MICE] Starting imputation round 98/110, elapsed time 147.371\n",
      "[MICE] Starting imputation round 99/110, elapsed time 148.903\n",
      "[MICE] Starting imputation round 100/110, elapsed time 150.508\n",
      "[MICE] Starting imputation round 101/110, elapsed time 152.134\n",
      "[MICE] Starting imputation round 102/110, elapsed time 153.761\n",
      "[MICE] Starting imputation round 103/110, elapsed time 155.254\n",
      "[MICE] Starting imputation round 104/110, elapsed time 156.708\n",
      "[MICE] Starting imputation round 105/110, elapsed time 158.159\n",
      "[MICE] Starting imputation round 106/110, elapsed time 159.613\n",
      "[MICE] Starting imputation round 107/110, elapsed time 161.165\n",
      "[MICE] Starting imputation round 108/110, elapsed time 162.772\n",
      "[MICE] Starting imputation round 109/110, elapsed time 164.227\n",
      "[MICE] Starting imputation round 110/110, elapsed time 165.676\n",
      "Peptides to remove: 4: ['EVADRVIFM', 'EVAESVMFM', 'KQWGWFALL', 'REMGIVDLL']\n",
      "After dropping similar peptides, test size 67 -> 65\n",
      "Dropping 12238 peptides with <2 observations\n",
      "Dropping 9 alleles with <2 observations: ['ELA-A1', 'HLA-B2701', 'HLA-B3508', 'HLA-B44', 'HLA-E0101', 'Mamu-B04', 'Patr-A0602', 'Patr-B0901', 'Patr-B1701']\n",
      "[MICE] Completing matrix with shape (19301, 97)\n",
      "[MICE] Starting imputation round 1/110, elapsed time 0.021\n",
      "[MICE] Starting imputation round 2/110, elapsed time 1.550\n",
      "[MICE] Starting imputation round 3/110, elapsed time 3.027\n",
      "[MICE] Starting imputation round 4/110, elapsed time 4.541\n",
      "[MICE] Starting imputation round 5/110, elapsed time 6.047\n",
      "[MICE] Starting imputation round 6/110, elapsed time 7.557\n",
      "[MICE] Starting imputation round 7/110, elapsed time 9.053\n",
      "[MICE] Starting imputation round 8/110, elapsed time 10.551\n",
      "[MICE] Starting imputation round 9/110, elapsed time 12.048\n",
      "[MICE] Starting imputation round 10/110, elapsed time 13.542\n",
      "[MICE] Starting imputation round 11/110, elapsed time 15.009\n",
      "[MICE] Starting imputation round 12/110, elapsed time 16.513\n",
      "[MICE] Starting imputation round 13/110, elapsed time 18.025\n",
      "[MICE] Starting imputation round 14/110, elapsed time 19.529\n",
      "[MICE] Starting imputation round 15/110, elapsed time 21.225\n",
      "[MICE] Starting imputation round 16/110, elapsed time 23.046\n",
      "[MICE] Starting imputation round 17/110, elapsed time 24.872\n",
      "[MICE] Starting imputation round 18/110, elapsed time 26.704\n",
      "[MICE] Starting imputation round 19/110, elapsed time 28.525\n",
      "[MICE] Starting imputation round 20/110, elapsed time 30.359\n",
      "[MICE] Starting imputation round 21/110, elapsed time 32.186\n",
      "[MICE] Starting imputation round 22/110, elapsed time 34.006\n",
      "[MICE] Starting imputation round 23/110, elapsed time 35.836\n",
      "[MICE] Starting imputation round 24/110, elapsed time 37.666\n",
      "[MICE] Starting imputation round 25/110, elapsed time 39.494\n",
      "[MICE] Starting imputation round 26/110, elapsed time 41.329\n",
      "[MICE] Starting imputation round 27/110, elapsed time 43.157\n",
      "[MICE] Starting imputation round 28/110, elapsed time 44.989\n",
      "[MICE] Starting imputation round 29/110, elapsed time 46.815\n",
      "[MICE] Starting imputation round 30/110, elapsed time 48.640\n",
      "[MICE] Starting imputation round 31/110, elapsed time 50.484\n",
      "[MICE] Starting imputation round 32/110, elapsed time 52.330\n",
      "[MICE] Starting imputation round 33/110, elapsed time 54.173\n",
      "[MICE] Starting imputation round 34/110, elapsed time 56.004\n",
      "[MICE] Starting imputation round 35/110, elapsed time 57.838\n",
      "[MICE] Starting imputation round 36/110, elapsed time 59.660\n",
      "[MICE] Starting imputation round 37/110, elapsed time 61.487\n",
      "[MICE] Starting imputation round 38/110, elapsed time 63.320\n",
      "[MICE] Starting imputation round 39/110, elapsed time 65.152\n",
      "[MICE] Starting imputation round 40/110, elapsed time 66.987\n",
      "[MICE] Starting imputation round 41/110, elapsed time 68.818\n",
      "[MICE] Starting imputation round 42/110, elapsed time 70.647\n",
      "[MICE] Starting imputation round 43/110, elapsed time 72.480\n",
      "[MICE] Starting imputation round 44/110, elapsed time 74.326\n",
      "[MICE] Starting imputation round 45/110, elapsed time 76.154\n",
      "[MICE] Starting imputation round 46/110, elapsed time 77.994\n",
      "[MICE] Starting imputation round 47/110, elapsed time 79.830\n",
      "[MICE] Starting imputation round 48/110, elapsed time 81.664\n",
      "[MICE] Starting imputation round 49/110, elapsed time 83.498\n",
      "[MICE] Starting imputation round 50/110, elapsed time 85.337\n",
      "[MICE] Starting imputation round 51/110, elapsed time 87.161\n",
      "[MICE] Starting imputation round 52/110, elapsed time 89.003\n",
      "[MICE] Starting imputation round 53/110, elapsed time 90.837\n",
      "[MICE] Starting imputation round 54/110, elapsed time 92.679\n",
      "[MICE] Starting imputation round 55/110, elapsed time 94.511\n",
      "[MICE] Starting imputation round 56/110, elapsed time 96.346\n",
      "[MICE] Starting imputation round 57/110, elapsed time 98.175\n",
      "[MICE] Starting imputation round 58/110, elapsed time 100.039\n",
      "[MICE] Starting imputation round 59/110, elapsed time 101.889\n",
      "[MICE] Starting imputation round 60/110, elapsed time 103.726\n",
      "[MICE] Starting imputation round 61/110, elapsed time 105.548\n",
      "[MICE] Starting imputation round 62/110, elapsed time 107.384\n",
      "[MICE] Starting imputation round 63/110, elapsed time 109.213\n",
      "[MICE] Starting imputation round 64/110, elapsed time 111.054\n",
      "[MICE] Starting imputation round 65/110, elapsed time 112.884\n",
      "[MICE] Starting imputation round 66/110, elapsed time 114.721\n",
      "[MICE] Starting imputation round 67/110, elapsed time 116.551\n",
      "[MICE] Starting imputation round 68/110, elapsed time 118.389\n",
      "[MICE] Starting imputation round 69/110, elapsed time 120.219\n",
      "[MICE] Starting imputation round 70/110, elapsed time 122.055\n",
      "[MICE] Starting imputation round 71/110, elapsed time 123.880\n",
      "[MICE] Starting imputation round 72/110, elapsed time 125.719\n",
      "[MICE] Starting imputation round 73/110, elapsed time 127.548\n",
      "[MICE] Starting imputation round 74/110, elapsed time 129.385\n",
      "[MICE] Starting imputation round 75/110, elapsed time 131.213\n",
      "[MICE] Starting imputation round 76/110, elapsed time 133.047\n",
      "[MICE] Starting imputation round 77/110, elapsed time 134.879\n",
      "[MICE] Starting imputation round 78/110, elapsed time 136.714\n",
      "[MICE] Starting imputation round 79/110, elapsed time 138.542\n",
      "[MICE] Starting imputation round 80/110, elapsed time 140.377\n",
      "[MICE] Starting imputation round 81/110, elapsed time 142.208\n",
      "[MICE] Starting imputation round 82/110, elapsed time 144.035\n",
      "[MICE] Starting imputation round 83/110, elapsed time 145.861\n",
      "[MICE] Starting imputation round 84/110, elapsed time 147.697\n",
      "[MICE] Starting imputation round 85/110, elapsed time 149.550\n",
      "[MICE] Starting imputation round 86/110, elapsed time 151.399\n",
      "[MICE] Starting imputation round 87/110, elapsed time 153.239\n",
      "[MICE] Starting imputation round 88/110, elapsed time 155.071\n",
      "[MICE] Starting imputation round 89/110, elapsed time 156.897\n",
      "[MICE] Starting imputation round 90/110, elapsed time 158.730\n",
      "[MICE] Starting imputation round 91/110, elapsed time 160.556\n",
      "[MICE] Starting imputation round 92/110, elapsed time 162.388\n",
      "[MICE] Starting imputation round 93/110, elapsed time 164.215\n",
      "[MICE] Starting imputation round 94/110, elapsed time 166.047\n",
      "[MICE] Starting imputation round 95/110, elapsed time 167.872\n",
      "[MICE] Starting imputation round 96/110, elapsed time 169.700\n",
      "[MICE] Starting imputation round 97/110, elapsed time 171.534\n",
      "[MICE] Starting imputation round 98/110, elapsed time 173.364\n",
      "[MICE] Starting imputation round 99/110, elapsed time 175.191\n",
      "[MICE] Starting imputation round 100/110, elapsed time 177.030\n",
      "[MICE] Starting imputation round 101/110, elapsed time 178.866\n",
      "[MICE] Starting imputation round 102/110, elapsed time 180.702\n",
      "[MICE] Starting imputation round 103/110, elapsed time 182.535\n",
      "[MICE] Starting imputation round 104/110, elapsed time 184.361\n",
      "[MICE] Starting imputation round 105/110, elapsed time 186.193\n",
      "[MICE] Starting imputation round 106/110, elapsed time 188.022\n",
      "[MICE] Starting imputation round 107/110, elapsed time 189.853\n",
      "[MICE] Starting imputation round 108/110, elapsed time 191.682\n",
      "[MICE] Starting imputation round 109/110, elapsed time 193.520\n",
      "[MICE] Starting imputation round 110/110, elapsed time 195.354\n",
      "Allele: HLA-A2603\n",
      "Peptides to remove: 10: ['AQNAISTTF', 'EFKQILTDF', 'EYKKFIATF', 'KRMMIRYCL', 'KRMMVRHCL', 'RQAELSKAY', 'RRMATTFTF', 'RRYTRRISL', 'WESGAVLCV', 'YERGNIIIF']\n",
      "After dropping similar peptides, test size 69 -> 64\n",
      "Dropping 12229 peptides with <2 observations\n",
      "Dropping 9 alleles with <2 observations: ['ELA-A1', 'HLA-B2701', 'HLA-B3508', 'HLA-B44', 'HLA-E0101', 'Mamu-B04', 'Patr-A0602', 'Patr-B0901', 'Patr-B1701']\n",
      "[MICE] Completing matrix with shape (19304, 97)\n",
      "[MICE] Starting imputation round 1/110, elapsed time 0.020\n",
      "[MICE] Starting imputation round 2/110, elapsed time 1.506\n",
      "[MICE] Starting imputation round 3/110, elapsed time 2.945\n",
      "[MICE] Starting imputation round 4/110, elapsed time 4.438\n",
      "[MICE] Starting imputation round 5/110, elapsed time 5.967\n",
      "[MICE] Starting imputation round 6/110, elapsed time 7.488\n",
      "[MICE] Starting imputation round 7/110, elapsed time 8.993\n",
      "[MICE] Starting imputation round 8/110, elapsed time 10.433\n",
      "[MICE] Starting imputation round 9/110, elapsed time 11.885\n",
      "[MICE] Starting imputation round 10/110, elapsed time 13.341\n",
      "[MICE] Starting imputation round 11/110, elapsed time 14.781\n",
      "[MICE] Starting imputation round 12/110, elapsed time 16.252\n",
      "[MICE] Starting imputation round 13/110, elapsed time 17.756\n",
      "[MICE] Starting imputation round 14/110, elapsed time 19.284\n",
      "[MICE] Starting imputation round 15/110, elapsed time 20.726\n",
      "[MICE] Starting imputation round 16/110, elapsed time 22.272\n",
      "[MICE] Starting imputation round 17/110, elapsed time 23.793\n",
      "[MICE] Starting imputation round 18/110, elapsed time 25.305\n",
      "[MICE] Starting imputation round 19/110, elapsed time 26.845\n",
      "[MICE] Starting imputation round 20/110, elapsed time 28.285\n",
      "[MICE] Starting imputation round 21/110, elapsed time 29.723\n",
      "[MICE] Starting imputation round 22/110, elapsed time 31.159\n",
      "[MICE] Starting imputation round 23/110, elapsed time 32.596\n",
      "[MICE] Starting imputation round 24/110, elapsed time 34.123\n",
      "[MICE] Starting imputation round 25/110, elapsed time 35.573\n",
      "[MICE] Starting imputation round 26/110, elapsed time 37.027\n",
      "[MICE] Starting imputation round 27/110, elapsed time 38.548\n",
      "[MICE] Starting imputation round 28/110, elapsed time 40.075\n",
      "[MICE] Starting imputation round 29/110, elapsed time 41.542\n",
      "[MICE] Starting imputation round 30/110, elapsed time 42.974\n",
      "[MICE] Starting imputation round 31/110, elapsed time 44.425\n",
      "[MICE] Starting imputation round 32/110, elapsed time 45.854\n",
      "[MICE] Starting imputation round 33/110, elapsed time 47.284\n",
      "[MICE] Starting imputation round 34/110, elapsed time 48.780\n",
      "[MICE] Starting imputation round 35/110, elapsed time 50.214\n",
      "[MICE] Starting imputation round 36/110, elapsed time 51.681\n",
      "[MICE] Starting imputation round 37/110, elapsed time 53.115\n",
      "[MICE] Starting imputation round 38/110, elapsed time 54.576\n",
      "[MICE] Starting imputation round 39/110, elapsed time 56.007\n",
      "[MICE] Starting imputation round 40/110, elapsed time 57.454\n",
      "[MICE] Starting imputation round 41/110, elapsed time 58.882\n",
      "[MICE] Starting imputation round 42/110, elapsed time 60.328\n",
      "[MICE] Starting imputation round 43/110, elapsed time 61.767\n",
      "[MICE] Starting imputation round 44/110, elapsed time 63.202\n",
      "[MICE] Starting imputation round 45/110, elapsed time 64.696\n",
      "[MICE] Starting imputation round 46/110, elapsed time 66.223\n",
      "[MICE] Starting imputation round 47/110, elapsed time 67.749\n",
      "[MICE] Starting imputation round 48/110, elapsed time 69.252\n",
      "[MICE] Starting imputation round 49/110, elapsed time 70.692\n",
      "[MICE] Starting imputation round 50/110, elapsed time 72.129\n",
      "[MICE] Starting imputation round 51/110, elapsed time 73.561\n",
      "[MICE] Starting imputation round 52/110, elapsed time 75.000\n",
      "[MICE] Starting imputation round 53/110, elapsed time 76.435\n",
      "[MICE] Starting imputation round 54/110, elapsed time 77.869\n",
      "[MICE] Starting imputation round 55/110, elapsed time 79.300\n",
      "[MICE] Starting imputation round 56/110, elapsed time 80.737\n",
      "[MICE] Starting imputation round 57/110, elapsed time 82.173\n",
      "[MICE] Starting imputation round 58/110, elapsed time 83.612\n",
      "[MICE] Starting imputation round 59/110, elapsed time 85.048\n",
      "[MICE] Starting imputation round 60/110, elapsed time 86.486\n",
      "[MICE] Starting imputation round 61/110, elapsed time 87.926\n",
      "[MICE] Starting imputation round 62/110, elapsed time 89.356\n",
      "[MICE] Starting imputation round 63/110, elapsed time 90.787\n",
      "[MICE] Starting imputation round 64/110, elapsed time 92.220\n",
      "[MICE] Starting imputation round 65/110, elapsed time 93.651\n",
      "[MICE] Starting imputation round 66/110, elapsed time 95.082\n",
      "[MICE] Starting imputation round 67/110, elapsed time 96.516\n",
      "[MICE] Starting imputation round 68/110, elapsed time 97.946\n",
      "[MICE] Starting imputation round 69/110, elapsed time 99.379\n",
      "[MICE] Starting imputation round 70/110, elapsed time 100.814\n",
      "[MICE] Starting imputation round 71/110, elapsed time 102.246\n",
      "[MICE] Starting imputation round 72/110, elapsed time 103.681\n",
      "[MICE] Starting imputation round 73/110, elapsed time 105.115\n",
      "[MICE] Starting imputation round 74/110, elapsed time 106.548\n",
      "[MICE] Starting imputation round 75/110, elapsed time 107.982\n",
      "[MICE] Starting imputation round 76/110, elapsed time 109.414\n",
      "[MICE] Starting imputation round 77/110, elapsed time 110.845\n",
      "[MICE] Starting imputation round 78/110, elapsed time 112.278\n",
      "[MICE] Starting imputation round 79/110, elapsed time 113.714\n",
      "[MICE] Starting imputation round 80/110, elapsed time 115.163\n",
      "[MICE] Starting imputation round 81/110, elapsed time 116.613\n",
      "[MICE] Starting imputation round 82/110, elapsed time 118.064\n",
      "[MICE] Starting imputation round 83/110, elapsed time 119.497\n",
      "[MICE] Starting imputation round 84/110, elapsed time 120.932\n",
      "[MICE] Starting imputation round 85/110, elapsed time 122.370\n",
      "[MICE] Starting imputation round 86/110, elapsed time 123.803\n",
      "[MICE] Starting imputation round 87/110, elapsed time 125.236\n",
      "[MICE] Starting imputation round 88/110, elapsed time 126.669\n",
      "[MICE] Starting imputation round 89/110, elapsed time 128.099\n",
      "[MICE] Starting imputation round 90/110, elapsed time 129.528\n",
      "[MICE] Starting imputation round 91/110, elapsed time 130.961\n",
      "[MICE] Starting imputation round 92/110, elapsed time 132.394\n",
      "[MICE] Starting imputation round 93/110, elapsed time 133.826\n",
      "[MICE] Starting imputation round 94/110, elapsed time 135.260\n",
      "[MICE] Starting imputation round 95/110, elapsed time 136.693\n",
      "[MICE] Starting imputation round 96/110, elapsed time 138.128\n",
      "[MICE] Starting imputation round 97/110, elapsed time 139.561\n",
      "[MICE] Starting imputation round 98/110, elapsed time 140.990\n",
      "[MICE] Starting imputation round 99/110, elapsed time 142.420\n",
      "[MICE] Starting imputation round 100/110, elapsed time 143.860\n",
      "[MICE] Starting imputation round 101/110, elapsed time 145.296\n",
      "[MICE] Starting imputation round 102/110, elapsed time 146.733\n",
      "[MICE] Starting imputation round 103/110, elapsed time 148.173\n",
      "[MICE] Starting imputation round 104/110, elapsed time 149.610\n",
      "[MICE] Starting imputation round 105/110, elapsed time 151.045\n",
      "[MICE] Starting imputation round 106/110, elapsed time 152.486\n",
      "[MICE] Starting imputation round 107/110, elapsed time 153.927\n",
      "[MICE] Starting imputation round 108/110, elapsed time 155.363\n",
      "[MICE] Starting imputation round 109/110, elapsed time 156.802\n",
      "[MICE] Starting imputation round 110/110, elapsed time 158.236\n",
      "Peptides to remove: 14: ['DIRQDVIAM', 'EFKQILTDF', 'EYKKFIATF', 'FLMRNAIQY', 'FYLFTFTIY', 'IHSDQLSKF', 'IIYERDFSY', 'IVHVDHECF', 'KRMMIRYCL', 'KRMMVRHCL', 'RIKTRLFTI', 'RRMATTFTF', 'RRYTRRISL', 'WHQARFEEL']\n",
      "After dropping similar peptides, test size 68 -> 61\n",
      "Dropping 12229 peptides with <2 observations\n",
      "Dropping 9 alleles with <2 observations: ['ELA-A1', 'HLA-B2701', 'HLA-B3508', 'HLA-B44', 'HLA-E0101', 'Mamu-B04', 'Patr-A0602', 'Patr-B0901', 'Patr-B1701']\n",
      "[MICE] Completing matrix with shape (19304, 97)\n",
      "[MICE] Starting imputation round 1/110, elapsed time 0.020\n",
      "[MICE] Starting imputation round 2/110, elapsed time 1.498\n",
      "[MICE] Starting imputation round 3/110, elapsed time 2.920\n",
      "[MICE] Starting imputation round 4/110, elapsed time 4.338\n",
      "[MICE] Starting imputation round 5/110, elapsed time 5.760\n",
      "[MICE] Starting imputation round 6/110, elapsed time 7.177\n",
      "[MICE] Starting imputation round 7/110, elapsed time 8.524\n",
      "[MICE] Starting imputation round 8/110, elapsed time 9.946\n",
      "[MICE] Starting imputation round 9/110, elapsed time 11.368\n",
      "[MICE] Starting imputation round 10/110, elapsed time 12.791\n",
      "[MICE] Starting imputation round 11/110, elapsed time 14.213\n",
      "[MICE] Starting imputation round 12/110, elapsed time 15.642\n",
      "[MICE] Starting imputation round 13/110, elapsed time 17.072\n",
      "[MICE] Starting imputation round 14/110, elapsed time 18.499\n",
      "[MICE] Starting imputation round 15/110, elapsed time 19.925\n",
      "[MICE] Starting imputation round 16/110, elapsed time 21.349\n",
      "[MICE] Starting imputation round 17/110, elapsed time 22.776\n",
      "[MICE] Starting imputation round 18/110, elapsed time 24.206\n",
      "[MICE] Starting imputation round 19/110, elapsed time 25.642\n",
      "[MICE] Starting imputation round 20/110, elapsed time 27.073\n",
      "[MICE] Starting imputation round 21/110, elapsed time 28.499\n",
      "[MICE] Starting imputation round 22/110, elapsed time 29.926\n",
      "[MICE] Starting imputation round 23/110, elapsed time 31.354\n",
      "[MICE] Starting imputation round 24/110, elapsed time 32.785\n",
      "[MICE] Starting imputation round 25/110, elapsed time 34.205\n",
      "[MICE] Starting imputation round 26/110, elapsed time 35.630\n",
      "[MICE] Starting imputation round 27/110, elapsed time 37.054\n",
      "[MICE] Starting imputation round 28/110, elapsed time 38.481\n",
      "[MICE] Starting imputation round 29/110, elapsed time 39.917\n",
      "[MICE] Starting imputation round 30/110, elapsed time 41.351\n",
      "[MICE] Starting imputation round 31/110, elapsed time 42.779\n",
      "[MICE] Starting imputation round 32/110, elapsed time 44.209\n",
      "[MICE] Starting imputation round 33/110, elapsed time 45.648\n",
      "[MICE] Starting imputation round 34/110, elapsed time 47.076\n",
      "[MICE] Starting imputation round 35/110, elapsed time 48.516\n",
      "[MICE] Starting imputation round 36/110, elapsed time 49.948\n",
      "[MICE] Starting imputation round 37/110, elapsed time 51.380\n",
      "[MICE] Starting imputation round 38/110, elapsed time 52.816\n",
      "[MICE] Starting imputation round 39/110, elapsed time 54.254\n",
      "[MICE] Starting imputation round 40/110, elapsed time 55.686\n",
      "[MICE] Starting imputation round 41/110, elapsed time 57.120\n",
      "[MICE] Starting imputation round 42/110, elapsed time 58.549\n",
      "[MICE] Starting imputation round 43/110, elapsed time 59.980\n",
      "[MICE] Starting imputation round 44/110, elapsed time 61.407\n",
      "[MICE] Starting imputation round 45/110, elapsed time 62.842\n",
      "[MICE] Starting imputation round 46/110, elapsed time 64.288\n",
      "[MICE] Starting imputation round 47/110, elapsed time 65.731\n",
      "[MICE] Starting imputation round 48/110, elapsed time 67.174\n",
      "[MICE] Starting imputation round 49/110, elapsed time 68.611\n",
      "[MICE] Starting imputation round 50/110, elapsed time 70.038\n",
      "[MICE] Starting imputation round 51/110, elapsed time 71.470\n",
      "[MICE] Starting imputation round 52/110, elapsed time 72.902\n",
      "[MICE] Starting imputation round 53/110, elapsed time 74.335\n",
      "[MICE] Starting imputation round 54/110, elapsed time 75.766\n",
      "[MICE] Starting imputation round 55/110, elapsed time 77.197\n",
      "[MICE] Starting imputation round 56/110, elapsed time 78.630\n",
      "[MICE] Starting imputation round 57/110, elapsed time 80.069\n",
      "[MICE] Starting imputation round 58/110, elapsed time 81.503\n",
      "[MICE] Starting imputation round 59/110, elapsed time 82.936\n",
      "[MICE] Starting imputation round 60/110, elapsed time 84.375\n",
      "[MICE] Starting imputation round 61/110, elapsed time 85.813\n",
      "[MICE] Starting imputation round 62/110, elapsed time 87.249\n",
      "[MICE] Starting imputation round 63/110, elapsed time 88.680\n",
      "[MICE] Starting imputation round 64/110, elapsed time 90.117\n",
      "[MICE] Starting imputation round 65/110, elapsed time 91.561\n",
      "[MICE] Starting imputation round 66/110, elapsed time 93.001\n",
      "[MICE] Starting imputation round 67/110, elapsed time 94.433\n",
      "[MICE] Starting imputation round 68/110, elapsed time 95.862\n",
      "[MICE] Starting imputation round 69/110, elapsed time 97.290\n",
      "[MICE] Starting imputation round 70/110, elapsed time 98.721\n",
      "[MICE] Starting imputation round 71/110, elapsed time 100.153\n",
      "[MICE] Starting imputation round 72/110, elapsed time 101.586\n",
      "[MICE] Starting imputation round 73/110, elapsed time 103.016\n",
      "[MICE] Starting imputation round 74/110, elapsed time 104.449\n",
      "[MICE] Starting imputation round 75/110, elapsed time 105.887\n",
      "[MICE] Starting imputation round 76/110, elapsed time 107.317\n",
      "[MICE] Starting imputation round 77/110, elapsed time 108.753\n",
      "[MICE] Starting imputation round 78/110, elapsed time 110.190\n",
      "[MICE] Starting imputation round 79/110, elapsed time 111.630\n",
      "[MICE] Starting imputation round 80/110, elapsed time 113.060\n",
      "[MICE] Starting imputation round 81/110, elapsed time 114.497\n",
      "[MICE] Starting imputation round 82/110, elapsed time 115.938\n",
      "[MICE] Starting imputation round 83/110, elapsed time 117.375\n",
      "[MICE] Starting imputation round 84/110, elapsed time 118.814\n",
      "[MICE] Starting imputation round 85/110, elapsed time 120.245\n",
      "[MICE] Starting imputation round 86/110, elapsed time 121.675\n",
      "[MICE] Starting imputation round 87/110, elapsed time 123.112\n",
      "[MICE] Starting imputation round 88/110, elapsed time 124.545\n",
      "[MICE] Starting imputation round 89/110, elapsed time 125.979\n",
      "[MICE] Starting imputation round 90/110, elapsed time 127.411\n",
      "[MICE] Starting imputation round 91/110, elapsed time 128.844\n",
      "[MICE] Starting imputation round 92/110, elapsed time 130.275\n",
      "[MICE] Starting imputation round 93/110, elapsed time 131.716\n",
      "[MICE] Starting imputation round 94/110, elapsed time 133.147\n",
      "[MICE] Starting imputation round 95/110, elapsed time 134.588\n",
      "[MICE] Starting imputation round 96/110, elapsed time 136.020\n",
      "[MICE] Starting imputation round 97/110, elapsed time 137.454\n",
      "[MICE] Starting imputation round 98/110, elapsed time 138.887\n",
      "[MICE] Starting imputation round 99/110, elapsed time 140.318\n",
      "[MICE] Starting imputation round 100/110, elapsed time 141.747\n",
      "[MICE] Starting imputation round 101/110, elapsed time 143.180\n",
      "[MICE] Starting imputation round 102/110, elapsed time 144.610\n",
      "[MICE] Starting imputation round 103/110, elapsed time 146.041\n",
      "[MICE] Starting imputation round 104/110, elapsed time 147.472\n",
      "[MICE] Starting imputation round 105/110, elapsed time 148.905\n",
      "[MICE] Starting imputation round 106/110, elapsed time 150.335\n",
      "[MICE] Starting imputation round 107/110, elapsed time 151.770\n",
      "[MICE] Starting imputation round 108/110, elapsed time 153.200\n",
      "[MICE] Starting imputation round 109/110, elapsed time 154.638\n",
      "[MICE] Starting imputation round 110/110, elapsed time 156.071\n",
      "Peptides to remove: 12: ['AQNAISTTF', 'DIRQDVIAM', 'FLMRNAIQY', 'FYLFTFTIY', 'IHSDQLSKF', 'IIYERDFSY', 'IVHVDHECF', 'RIKTRLFTI', 'RQAELSKAY', 'WESGAVLCV', 'WHQARFEEL', 'YERGNIIIF']\n",
      "After dropping similar peptides, test size 68 -> 62\n",
      "Dropping 12234 peptides with <2 observations\n",
      "Dropping 9 alleles with <2 observations: ['ELA-A1', 'HLA-B2701', 'HLA-B3508', 'HLA-B44', 'HLA-E0101', 'Mamu-B04', 'Patr-A0602', 'Patr-B0901', 'Patr-B1701']\n",
      "[MICE] Completing matrix with shape (19304, 97)\n",
      "[MICE] Starting imputation round 1/110, elapsed time 0.020\n",
      "[MICE] Starting imputation round 2/110, elapsed time 1.493\n",
      "[MICE] Starting imputation round 3/110, elapsed time 2.912\n",
      "[MICE] Starting imputation round 4/110, elapsed time 4.331\n",
      "[MICE] Starting imputation round 5/110, elapsed time 5.750\n",
      "[MICE] Starting imputation round 6/110, elapsed time 7.169\n",
      "[MICE] Starting imputation round 7/110, elapsed time 8.587\n",
      "[MICE] Starting imputation round 8/110, elapsed time 10.006\n",
      "[MICE] Starting imputation round 9/110, elapsed time 11.424\n",
      "[MICE] Starting imputation round 10/110, elapsed time 12.843\n",
      "[MICE] Starting imputation round 11/110, elapsed time 14.263\n",
      "[MICE] Starting imputation round 12/110, elapsed time 15.691\n",
      "[MICE] Starting imputation round 13/110, elapsed time 17.137\n",
      "[MICE] Starting imputation round 14/110, elapsed time 18.583\n",
      "[MICE] Starting imputation round 15/110, elapsed time 20.005\n",
      "[MICE] Starting imputation round 16/110, elapsed time 21.423\n",
      "[MICE] Starting imputation round 17/110, elapsed time 22.841\n",
      "[MICE] Starting imputation round 18/110, elapsed time 24.267\n",
      "[MICE] Starting imputation round 19/110, elapsed time 25.701\n",
      "[MICE] Starting imputation round 20/110, elapsed time 27.128\n",
      "[MICE] Starting imputation round 21/110, elapsed time 28.554\n",
      "[MICE] Starting imputation round 22/110, elapsed time 29.983\n",
      "[MICE] Starting imputation round 23/110, elapsed time 31.407\n",
      "[MICE] Starting imputation round 24/110, elapsed time 32.826\n",
      "[MICE] Starting imputation round 25/110, elapsed time 34.248\n",
      "[MICE] Starting imputation round 26/110, elapsed time 35.674\n",
      "[MICE] Starting imputation round 27/110, elapsed time 37.103\n",
      "[MICE] Starting imputation round 28/110, elapsed time 38.548\n",
      "[MICE] Starting imputation round 29/110, elapsed time 39.987\n",
      "[MICE] Starting imputation round 30/110, elapsed time 41.422\n",
      "[MICE] Starting imputation round 31/110, elapsed time 42.870\n",
      "[MICE] Starting imputation round 32/110, elapsed time 44.316\n",
      "[MICE] Starting imputation round 33/110, elapsed time 45.750\n",
      "[MICE] Starting imputation round 34/110, elapsed time 47.188\n",
      "[MICE] Starting imputation round 35/110, elapsed time 48.624\n",
      "[MICE] Starting imputation round 36/110, elapsed time 50.059\n",
      "[MICE] Starting imputation round 37/110, elapsed time 51.496\n",
      "[MICE] Starting imputation round 38/110, elapsed time 52.932\n",
      "[MICE] Starting imputation round 39/110, elapsed time 54.367\n",
      "[MICE] Starting imputation round 40/110, elapsed time 55.802\n",
      "[MICE] Starting imputation round 41/110, elapsed time 57.246\n",
      "[MICE] Starting imputation round 42/110, elapsed time 58.683\n",
      "[MICE] Starting imputation round 43/110, elapsed time 60.124\n",
      "[MICE] Starting imputation round 44/110, elapsed time 61.561\n",
      "[MICE] Starting imputation round 45/110, elapsed time 63.005\n",
      "[MICE] Starting imputation round 46/110, elapsed time 64.446\n",
      "[MICE] Starting imputation round 47/110, elapsed time 65.881\n",
      "[MICE] Starting imputation round 48/110, elapsed time 67.316\n",
      "[MICE] Starting imputation round 49/110, elapsed time 68.751\n",
      "[MICE] Starting imputation round 50/110, elapsed time 70.190\n",
      "[MICE] Starting imputation round 51/110, elapsed time 71.632\n",
      "[MICE] Starting imputation round 52/110, elapsed time 73.071\n",
      "[MICE] Starting imputation round 53/110, elapsed time 74.506\n",
      "[MICE] Starting imputation round 54/110, elapsed time 75.942\n",
      "[MICE] Starting imputation round 55/110, elapsed time 77.378\n",
      "[MICE] Starting imputation round 56/110, elapsed time 78.815\n",
      "[MICE] Starting imputation round 57/110, elapsed time 80.249\n",
      "[MICE] Starting imputation round 58/110, elapsed time 81.679\n",
      "[MICE] Starting imputation round 59/110, elapsed time 83.115\n",
      "[MICE] Starting imputation round 60/110, elapsed time 84.555\n",
      "[MICE] Starting imputation round 61/110, elapsed time 85.996\n",
      "[MICE] Starting imputation round 62/110, elapsed time 87.435\n",
      "[MICE] Starting imputation round 63/110, elapsed time 88.872\n",
      "[MICE] Starting imputation round 64/110, elapsed time 90.312\n",
      "[MICE] Starting imputation round 65/110, elapsed time 91.748\n",
      "[MICE] Starting imputation round 66/110, elapsed time 93.188\n",
      "[MICE] Starting imputation round 67/110, elapsed time 94.623\n",
      "[MICE] Starting imputation round 68/110, elapsed time 96.061\n",
      "[MICE] Starting imputation round 69/110, elapsed time 97.495\n",
      "[MICE] Starting imputation round 70/110, elapsed time 98.927\n",
      "[MICE] Starting imputation round 71/110, elapsed time 100.362\n",
      "[MICE] Starting imputation round 72/110, elapsed time 101.798\n",
      "[MICE] Starting imputation round 73/110, elapsed time 103.232\n",
      "[MICE] Starting imputation round 74/110, elapsed time 104.668\n",
      "[MICE] Starting imputation round 75/110, elapsed time 106.105\n",
      "[MICE] Starting imputation round 76/110, elapsed time 107.536\n",
      "[MICE] Starting imputation round 77/110, elapsed time 108.967\n",
      "[MICE] Starting imputation round 78/110, elapsed time 110.400\n",
      "[MICE] Starting imputation round 79/110, elapsed time 111.826\n",
      "[MICE] Starting imputation round 80/110, elapsed time 113.254\n",
      "[MICE] Starting imputation round 81/110, elapsed time 114.686\n",
      "[MICE] Starting imputation round 82/110, elapsed time 116.117\n",
      "[MICE] Starting imputation round 83/110, elapsed time 117.546\n",
      "[MICE] Starting imputation round 84/110, elapsed time 118.978\n",
      "[MICE] Starting imputation round 85/110, elapsed time 120.407\n",
      "[MICE] Starting imputation round 86/110, elapsed time 121.843\n",
      "[MICE] Starting imputation round 87/110, elapsed time 123.275\n",
      "[MICE] Starting imputation round 88/110, elapsed time 124.712\n",
      "[MICE] Starting imputation round 89/110, elapsed time 126.148\n",
      "[MICE] Starting imputation round 90/110, elapsed time 127.582\n",
      "[MICE] Starting imputation round 91/110, elapsed time 129.015\n",
      "[MICE] Starting imputation round 92/110, elapsed time 130.448\n",
      "[MICE] Starting imputation round 93/110, elapsed time 131.880\n",
      "[MICE] Starting imputation round 94/110, elapsed time 133.315\n",
      "[MICE] Starting imputation round 95/110, elapsed time 134.748\n",
      "[MICE] Starting imputation round 96/110, elapsed time 136.177\n",
      "[MICE] Starting imputation round 97/110, elapsed time 137.609\n",
      "[MICE] Starting imputation round 98/110, elapsed time 139.046\n",
      "[MICE] Starting imputation round 99/110, elapsed time 140.480\n",
      "[MICE] Starting imputation round 100/110, elapsed time 141.909\n",
      "[MICE] Starting imputation round 101/110, elapsed time 143.346\n",
      "[MICE] Starting imputation round 102/110, elapsed time 144.793\n",
      "[MICE] Starting imputation round 103/110, elapsed time 146.245\n",
      "[MICE] Starting imputation round 104/110, elapsed time 147.705\n",
      "[MICE] Starting imputation round 105/110, elapsed time 149.144\n",
      "[MICE] Starting imputation round 106/110, elapsed time 150.585\n",
      "[MICE] Starting imputation round 107/110, elapsed time 152.019\n",
      "[MICE] Starting imputation round 108/110, elapsed time 153.450\n",
      "[MICE] Starting imputation round 109/110, elapsed time 154.881\n",
      "[MICE] Starting imputation round 110/110, elapsed time 156.311\n"
     ]
    }
   ],
   "source": [
    "# allele -> list of (train set, imputed train set, test set) for each fold\n",
    "n_folds = 3\n",
    "cv_splits = {}\n",
    "for allele in alleles:\n",
    "    print(\"Allele: %s\" % allele)\n",
    "    cv_iter = all_train_data.cross_validation_iterator(allele, n_folds=n_folds, shuffle=True)\n",
    "    triples = []\n",
    "    for (all_allele_train_split, full_test_split) in cv_iter:\n",
    "        peptides_to_remove = similar_peptides(\n",
    "            all_allele_train_split.get_allele(allele).peptides,\n",
    "            full_test_split.get_allele(allele).peptides\n",
    "        )\n",
    "        print(\"Peptides to remove: %d: %s\" % (len(peptides_to_remove), str(peptides_to_remove)))\n",
    "        if peptides_to_remove:\n",
    "            test_split = full_test_split.drop_allele_peptide_lists(\n",
    "                [allele] * len(peptides_to_remove),\n",
    "                peptides_to_remove)\n",
    "            print(\"After dropping similar peptides, test size %d -> %d\" % (len(full_test_split), len(test_split)))\n",
    "        else:\n",
    "            test_split = full_test_split\n",
    "        imputed_train_split = all_allele_train_split.impute_missing_values(\n",
    "            imputer,\n",
    "            min_observations_per_peptide=2,\n",
    "            min_observations_per_allele=2).get_allele(allele)\n",
    "        train_split = all_allele_train_split.get_allele(allele)\n",
    "        triples.append((train_split, imputed_train_split, test_split))\n",
    "    cv_splits[allele] = triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allele: HLA-A0201\n",
      "test size 9555 -> 9492 after removing\n",
      "Dropping 10067 peptides with <2 observations\n",
      "Dropping 9 alleles with <2 observations: ['ELA-A1', 'HLA-B2701', 'HLA-B3508', 'HLA-B44', 'HLA-E0101', 'Mamu-B04', 'Patr-A0602', 'Patr-B0901', 'Patr-B1701']\n",
      "[MICE] Completing matrix with shape (18850, 97)\n",
      "[MICE] Starting imputation round 1/55, elapsed time 0.020\n",
      "[MICE] Starting imputation round 2/55, elapsed time 0.380\n",
      "[MICE] Starting imputation round 3/55, elapsed time 0.729\n",
      "[MICE] Starting imputation round 4/55, elapsed time 1.084\n",
      "[MICE] Starting imputation round 5/55, elapsed time 1.440\n",
      "[MICE] Starting imputation round 6/55, elapsed time 1.794\n",
      "[MICE] Starting imputation round 7/55, elapsed time 2.184\n",
      "[MICE] Starting imputation round 8/55, elapsed time 2.599\n",
      "[MICE] Starting imputation round 9/55, elapsed time 2.962\n",
      "[MICE] Starting imputation round 10/55, elapsed time 3.363\n",
      "[MICE] Starting imputation round 11/55, elapsed time 3.803\n",
      "[MICE] Starting imputation round 12/55, elapsed time 4.242\n",
      "[MICE] Starting imputation round 13/55, elapsed time 4.638\n",
      "[MICE] Starting imputation round 14/55, elapsed time 5.012\n",
      "[MICE] Starting imputation round 15/55, elapsed time 5.387\n",
      "[MICE] Starting imputation round 16/55, elapsed time 5.749\n",
      "[MICE] Starting imputation round 17/55, elapsed time 6.158\n",
      "[MICE] Starting imputation round 18/55, elapsed time 6.583\n",
      "[MICE] Starting imputation round 19/55, elapsed time 6.948\n",
      "[MICE] Starting imputation round 20/55, elapsed time 7.308\n",
      "[MICE] Starting imputation round 21/55, elapsed time 7.672\n",
      "[MICE] Starting imputation round 22/55, elapsed time 8.036\n",
      "[MICE] Starting imputation round 23/55, elapsed time 8.424\n",
      "[MICE] Starting imputation round 24/55, elapsed time 8.783\n",
      "[MICE] Starting imputation round 25/55, elapsed time 9.186\n",
      "[MICE] Starting imputation round 26/55, elapsed time 9.546\n",
      "[MICE] Starting imputation round 27/55, elapsed time 9.905\n",
      "[MICE] Starting imputation round 28/55, elapsed time 10.264\n",
      "[MICE] Starting imputation round 29/55, elapsed time 10.622\n",
      "[MICE] Starting imputation round 30/55, elapsed time 10.982\n",
      "[MICE] Starting imputation round 31/55, elapsed time 11.341\n",
      "[MICE] Starting imputation round 32/55, elapsed time 11.728\n",
      "[MICE] Starting imputation round 33/55, elapsed time 12.088\n",
      "[MICE] Starting imputation round 34/55, elapsed time 12.469\n",
      "[MICE] Starting imputation round 35/55, elapsed time 12.868\n",
      "[MICE] Starting imputation round 36/55, elapsed time 13.227\n",
      "[MICE] Starting imputation round 37/55, elapsed time 13.625\n",
      "[MICE] Starting imputation round 38/55, elapsed time 14.019\n",
      "[MICE] Starting imputation round 39/55, elapsed time 14.378\n",
      "[MICE] Starting imputation round 40/55, elapsed time 14.773\n",
      "[MICE] Starting imputation round 41/55, elapsed time 15.210\n",
      "[MICE] Starting imputation round 42/55, elapsed time 15.645\n",
      "[MICE] Starting imputation round 43/55, elapsed time 16.081\n",
      "[MICE] Starting imputation round 44/55, elapsed time 16.520\n",
      "[MICE] Starting imputation round 45/55, elapsed time 16.940\n",
      "[MICE] Starting imputation round 46/55, elapsed time 17.301\n",
      "[MICE] Starting imputation round 47/55, elapsed time 17.662\n",
      "[MICE] Starting imputation round 48/55, elapsed time 18.023\n",
      "[MICE] Starting imputation round 49/55, elapsed time 18.412\n",
      "[MICE] Starting imputation round 50/55, elapsed time 18.826\n",
      "[MICE] Starting imputation round 51/55, elapsed time 19.263\n",
      "[MICE] Starting imputation round 52/55, elapsed time 19.659\n",
      "[MICE] Starting imputation round 53/55, elapsed time 20.019\n",
      "[MICE] Starting imputation round 54/55, elapsed time 20.380\n",
      "[MICE] Starting imputation round 55/55, elapsed time 20.741\n",
      "test size 9315 -> 8482 after removing\n",
      "Dropping 10127 peptides with <2 observations\n",
      "Dropping 9 alleles with <2 observations: ['ELA-A1', 'HLA-B2701', 'HLA-B3508', 'HLA-B44', 'HLA-E0101', 'Mamu-B04', 'Patr-A0602', 'Patr-B0901', 'Patr-B1701']\n",
      "[MICE] Completing matrix with shape (18858, 97)\n",
      "[MICE] Starting imputation round 1/55, elapsed time 0.020\n",
      "[MICE] Starting imputation round 2/55, elapsed time 0.458\n",
      "[MICE] Starting imputation round 3/55, elapsed time 0.883\n",
      "[MICE] Starting imputation round 4/55, elapsed time 1.242\n",
      "[MICE] Starting imputation round 5/55, elapsed time 1.595\n",
      "[MICE] Starting imputation round 6/55, elapsed time 1.945\n",
      "[MICE] Starting imputation round 7/55, elapsed time 2.303\n",
      "[MICE] Starting imputation round 8/55, elapsed time 2.661\n",
      "[MICE] Starting imputation round 9/55, elapsed time 3.065\n",
      "[MICE] Starting imputation round 10/55, elapsed time 3.499\n",
      "[MICE] Starting imputation round 11/55, elapsed time 3.893\n",
      "[MICE] Starting imputation round 12/55, elapsed time 4.251\n",
      "[MICE] Starting imputation round 13/55, elapsed time 4.654\n",
      "[MICE] Starting imputation round 14/55, elapsed time 5.090\n",
      "[MICE] Starting imputation round 15/55, elapsed time 5.526\n",
      "[MICE] Starting imputation round 16/55, elapsed time 5.962\n",
      "[MICE] Starting imputation round 17/55, elapsed time 6.396\n",
      "[MICE] Starting imputation round 18/55, elapsed time 6.831\n",
      "[MICE] Starting imputation round 19/55, elapsed time 7.224\n",
      "[MICE] Starting imputation round 20/55, elapsed time 7.580\n",
      "[MICE] Starting imputation round 21/55, elapsed time 7.940\n",
      "[MICE] Starting imputation round 22/55, elapsed time 8.299\n",
      "[MICE] Starting imputation round 23/55, elapsed time 8.727\n",
      "[MICE] Starting imputation round 24/55, elapsed time 9.164\n",
      "[MICE] Starting imputation round 25/55, elapsed time 9.600\n",
      "[MICE] Starting imputation round 26/55, elapsed time 10.037\n",
      "[MICE] Starting imputation round 27/55, elapsed time 10.474\n",
      "[MICE] Starting imputation round 28/55, elapsed time 10.912\n",
      "[MICE] Starting imputation round 29/55, elapsed time 11.348\n",
      "[MICE] Starting imputation round 30/55, elapsed time 11.787\n",
      "[MICE] Starting imputation round 31/55, elapsed time 12.224\n",
      "[MICE] Starting imputation round 32/55, elapsed time 12.660\n",
      "[MICE] Starting imputation round 33/55, elapsed time 13.096\n",
      "[MICE] Starting imputation round 34/55, elapsed time 13.533\n",
      "[MICE] Starting imputation round 35/55, elapsed time 13.970\n",
      "[MICE] Starting imputation round 36/55, elapsed time 14.407\n",
      "[MICE] Starting imputation round 37/55, elapsed time 14.844\n",
      "[MICE] Starting imputation round 38/55, elapsed time 15.281\n",
      "[MICE] Starting imputation round 39/55, elapsed time 15.716\n",
      "[MICE] Starting imputation round 40/55, elapsed time 16.154\n",
      "[MICE] Starting imputation round 41/55, elapsed time 16.590\n",
      "[MICE] Starting imputation round 42/55, elapsed time 17.027\n",
      "[MICE] Starting imputation round 43/55, elapsed time 17.463\n",
      "[MICE] Starting imputation round 44/55, elapsed time 17.837\n",
      "[MICE] Starting imputation round 45/55, elapsed time 18.196\n",
      "[MICE] Starting imputation round 46/55, elapsed time 18.554\n",
      "[MICE] Starting imputation round 47/55, elapsed time 18.913\n",
      "[MICE] Starting imputation round 48/55, elapsed time 19.272\n",
      "[MICE] Starting imputation round 49/55, elapsed time 19.631\n",
      "[MICE] Starting imputation round 50/55, elapsed time 19.990\n",
      "[MICE] Starting imputation round 51/55, elapsed time 20.393\n",
      "[MICE] Starting imputation round 52/55, elapsed time 20.830\n",
      "[MICE] Starting imputation round 53/55, elapsed time 21.267\n",
      "[MICE] Starting imputation round 54/55, elapsed time 21.701\n",
      "[MICE] Starting imputation round 55/55, elapsed time 22.141\n",
      "test size 8565 -> 6532 after removing\n",
      "Dropping 10274 peptides with <2 observations\n",
      "Dropping 9 alleles with <2 observations: ['ELA-A1', 'HLA-B2701', 'HLA-B3508', 'HLA-B44', 'HLA-E0101', 'Mamu-B04', 'Patr-A0602', 'Patr-B0901', 'Patr-B1701']\n",
      "[MICE] Completing matrix with shape (18894, 97)\n",
      "[MICE] Starting imputation round 1/55, elapsed time 0.020\n",
      "[MICE] Starting imputation round 2/55, elapsed time 0.397\n",
      "[MICE] Starting imputation round 3/55, elapsed time 0.753\n",
      "[MICE] Starting imputation round 4/55, elapsed time 1.109\n",
      "[MICE] Starting imputation round 5/55, elapsed time 1.461\n",
      "[MICE] Starting imputation round 6/55, elapsed time 1.813\n",
      "[MICE] Starting imputation round 7/55, elapsed time 2.171\n",
      "[MICE] Starting imputation round 8/55, elapsed time 2.587\n",
      "[MICE] Starting imputation round 9/55, elapsed time 3.022\n",
      "[MICE] Starting imputation round 10/55, elapsed time 3.457\n",
      "[MICE] Starting imputation round 11/55, elapsed time 3.892\n",
      "[MICE] Starting imputation round 12/55, elapsed time 4.328\n",
      "[MICE] Starting imputation round 13/55, elapsed time 4.763\n",
      "[MICE] Starting imputation round 14/55, elapsed time 5.200\n",
      "[MICE] Starting imputation round 15/55, elapsed time 5.636\n",
      "[MICE] Starting imputation round 16/55, elapsed time 6.072\n",
      "[MICE] Starting imputation round 17/55, elapsed time 6.507\n",
      "[MICE] Starting imputation round 18/55, elapsed time 6.941\n",
      "[MICE] Starting imputation round 19/55, elapsed time 7.377\n",
      "[MICE] Starting imputation round 20/55, elapsed time 7.813\n",
      "[MICE] Starting imputation round 21/55, elapsed time 8.250\n",
      "[MICE] Starting imputation round 22/55, elapsed time 8.686\n",
      "[MICE] Starting imputation round 23/55, elapsed time 9.121\n",
      "[MICE] Starting imputation round 24/55, elapsed time 9.557\n",
      "[MICE] Starting imputation round 25/55, elapsed time 9.994\n",
      "[MICE] Starting imputation round 26/55, elapsed time 10.410\n",
      "[MICE] Starting imputation round 27/55, elapsed time 10.770\n",
      "[MICE] Starting imputation round 28/55, elapsed time 11.129\n",
      "[MICE] Starting imputation round 29/55, elapsed time 11.488\n",
      "[MICE] Starting imputation round 30/55, elapsed time 11.849\n",
      "[MICE] Starting imputation round 31/55, elapsed time 12.231\n",
      "[MICE] Starting imputation round 32/55, elapsed time 12.591\n",
      "[MICE] Starting imputation round 33/55, elapsed time 13.019\n",
      "[MICE] Starting imputation round 34/55, elapsed time 13.456\n",
      "[MICE] Starting imputation round 35/55, elapsed time 13.894\n",
      "[MICE] Starting imputation round 36/55, elapsed time 14.332\n",
      "[MICE] Starting imputation round 37/55, elapsed time 14.768\n",
      "[MICE] Starting imputation round 38/55, elapsed time 15.205\n",
      "[MICE] Starting imputation round 39/55, elapsed time 15.641\n",
      "[MICE] Starting imputation round 40/55, elapsed time 16.078\n",
      "[MICE] Starting imputation round 41/55, elapsed time 16.515\n",
      "[MICE] Starting imputation round 42/55, elapsed time 16.952\n",
      "[MICE] Starting imputation round 43/55, elapsed time 17.389\n",
      "[MICE] Starting imputation round 44/55, elapsed time 17.825\n",
      "[MICE] Starting imputation round 45/55, elapsed time 18.262\n",
      "[MICE] Starting imputation round 46/55, elapsed time 18.699\n",
      "[MICE] Starting imputation round 47/55, elapsed time 19.135\n",
      "[MICE] Starting imputation round 48/55, elapsed time 19.572\n",
      "[MICE] Starting imputation round 49/55, elapsed time 20.008\n",
      "[MICE] Starting imputation round 50/55, elapsed time 20.439\n",
      "[MICE] Starting imputation round 51/55, elapsed time 20.799\n",
      "[MICE] Starting imputation round 52/55, elapsed time 21.159\n",
      "[MICE] Starting imputation round 53/55, elapsed time 21.517\n",
      "[MICE] Starting imputation round 54/55, elapsed time 21.876\n",
      "[MICE] Starting imputation round 55/55, elapsed time 22.244\n"
     ]
    }
   ],
   "source": [
    "cv_splits_downsampled = {}\n",
    "for allele in [\"HLA-A0201\"]:\n",
    "    print(\"Allele: %s\" % allele)\n",
    "    for size in [10, 250, 1000]:\n",
    "        triples = []\n",
    "        for replica in range(1):\n",
    "            train_peptides = set(numpy.random.choice(all_train_data.get_allele(allele).peptides, size, replace=False))\n",
    "            full_test_peptides = [x for x in all_train_data.get_allele(allele).peptides if x not in train_peptides]\n",
    "            peptides_to_remove = set(similar_peptides(train_peptides, full_test_peptides))\n",
    "            test_peptides = [x for x in full_test_peptides if x not in peptides_to_remove]\n",
    "            #print(\"Removing peptides: %s\" % peptides_to_remove)\n",
    "            print(\"test size %d -> %d after removing\" % (len(full_test_peptides), len(test_peptides)))\n",
    "\n",
    "            train_dataset = all_train_data.drop_allele_peptide_lists(\n",
    "                [allele] * len(full_test_peptides), sorted(full_test_peptides))\n",
    "            test_dataset = all_train_data.drop_allele_peptide_pairs(\n",
    "                [(allele, peptide) for peptide in sorted(peptides_to_remove) + sorted(train_peptides)])\n",
    "\n",
    "            imputed_train_split = train_dataset.impute_missing_values(\n",
    "                imputer,\n",
    "                min_observations_per_peptide=2,\n",
    "                min_observations_per_allele=2)\n",
    "\n",
    "            triples.append((\n",
    "                    train_dataset.get_allele(allele),\n",
    "                    imputed_train_split.get_allele(allele),\n",
    "                    test_dataset.get_allele(allele)))\n",
    "        cv_splits_downsampled[\"%s-%d\" % (allele, size)] = triples\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HLA-A0201-10': [(Dataset(n=10, alleles=['HLA-A0201']),\n",
       "   Dataset(n=18850, alleles=['HLA-A0201']),\n",
       "   Dataset(n=9492, alleles=['HLA-A0201']))],\n",
       " 'HLA-A0201-1000': [(Dataset(n=1000, alleles=['HLA-A0201']),\n",
       "   Dataset(n=18894, alleles=['HLA-A0201']),\n",
       "   Dataset(n=6532, alleles=['HLA-A0201']))],\n",
       " 'HLA-A0201-250': [(Dataset(n=250, alleles=['HLA-A0201']),\n",
       "   Dataset(n=18858, alleles=['HLA-A0201']),\n",
       "   Dataset(n=8482, alleles=['HLA-A0201']))]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_splits_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_splits_downsampled[\"HLA-A0201-250\"][0][0]._df.peptide.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allele: HLA-A0201-1000\n",
      "-- fold #1/3\n",
      "HLA-A0201-1000 fold   0 [  0 /  48] train_size=1000 test_size=6532 impute=False model={'pretrain_decay': '1 / (1+epoch)**2', 'dropout_probability': 0.5, 'layer_sizes': [4], 'fraction_negative': 0.1, 'activation': 'tanh', 'impute': False, 'embedding_output_dim': 4, 'n_training_epochs': 250}\n"
     ]
    }
   ],
   "source": [
    "cv_df = defaultdict(list)\n",
    "start = time.time()\n",
    "\n",
    "for (allele, triples) in cv_splits_downsampled.items():\n",
    "    print(\"Allele: %s\" % allele)\n",
    "            \n",
    "    #cv = sklearn.cross_validation.LabelKFold(original_peptides, n_folds = 3)\n",
    "    for (fold_num, (subset_train, subset_impute, subset_test)) in enumerate(triples):\n",
    "        print(\"-- fold #%d/3\" % (fold_num + 1,))\n",
    "        \n",
    "        np.random.shuffle(models_params_list)\n",
    "        for (i, original_model_params) in enumerate(models_params_list):\n",
    "            model_params = dict(original_model_params)\n",
    "            fraction_negative = model_params[\"fraction_negative\"]\n",
    "            del model_params[\"fraction_negative\"]\n",
    "            \n",
    "            impute = model_params[\"impute\"]\n",
    "            del model_params[\"impute\"]\n",
    "            \n",
    "            n_training_epochs = model_params[\"n_training_epochs\"]\n",
    "            del model_params[\"n_training_epochs\"]\n",
    "            \n",
    "            pretrain_decay = model_params[\"pretrain_decay\"]\n",
    "            del model_params[\"pretrain_decay\"] \n",
    "            \n",
    "            print(\"%10s fold %3d [%3d / %3d] train_size=%d test_size=%d impute=%s model=%s\" %\n",
    "                  (allele, fold_num, i, len(models_params_list), len(subset_train), len(subset_test), impute, original_model_params))\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            predictor = mhcflurry.Class1BindingPredictor.from_hyperparameters(max_ic50=max_ic50, **model_params)\n",
    "\n",
    "            fit_time = -time.time()\n",
    "            predictor.fit_dataset(\n",
    "                subset_train,\n",
    "                pretrain_decay=lambda epoch: eval(pretrain_decay, {'epoch': epoch, 'numpy': numpy}),\n",
    "                pretraining_dataset=subset_impute if impute else None,\n",
    "                verbose=False,\n",
    "                batch_size=128,\n",
    "                n_training_epochs=n_training_epochs,\n",
    "                n_random_negative_samples=int(fraction_negative * len(subset_train)))\n",
    "            fit_time += time.time()\n",
    "            \n",
    "            train_predictions = predictor.predict(subset_train.peptides)\n",
    "            test_predictions = predictor.predict(subset_test.peptides)\n",
    "            \n",
    "            cv_df[\"allele\"].append(allele)\n",
    "            cv_df[\"train_size\"].append(len(subset_train))\n",
    "            cv_df[\"test_size\"].append(len(subset_test))\n",
    "\n",
    "            cv_df[\"model_params\"].append(original_model_params)\n",
    "            cv_df[\"fit_time\"].append(fit_time)\n",
    "\n",
    "            for (param, param_value) in original_model_params.items():\n",
    "                cv_df[param].append(param_value)\n",
    "            \n",
    "            for (key, value) in make_scores(subset_train.affinities, train_predictions).items():\n",
    "                cv_df[\"train_%s\" % key].append(value)\n",
    "                print(\"train %s: %f\" % (key, value))\n",
    "            \n",
    "            for (key, value) in make_scores(\n",
    "                    subset_test.affinities, \n",
    "                    test_predictions).items():\n",
    "                cv_df[\"test_%s\" % key].append(value)\n",
    "                print(\"test %s: %f\" % (key, value))\n",
    "            \n",
    "            best_model_index = sorted(cv_df)\n",
    "\n",
    "\n",
    "cv_df = pandas.DataFrame(cv_df)\n",
    "cv_df[\"layer0_size\"] = [x[0] for x in cv_df.layer_sizes]\n",
    "print(time.time() - start)\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>allele</th>\n",
       "      <th>dropout_probability</th>\n",
       "      <th>embedding_output_dim</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>fraction_negative</th>\n",
       "      <th>impute</th>\n",
       "      <th>layer_sizes</th>\n",
       "      <th>model_params</th>\n",
       "      <th>pretrain_decay</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_size</th>\n",
       "      <th>test_tau</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_size</th>\n",
       "      <th>train_tau</th>\n",
       "      <th>layer0_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>9.020235</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.944129</td>\n",
       "      <td>0.781538</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.575135</td>\n",
       "      <td>0.954311</td>\n",
       "      <td>0.826797</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.639283</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>21.483086</td>\n",
       "      <td>0.25</td>\n",
       "      <td>True</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.944119</td>\n",
       "      <td>0.788403</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.579707</td>\n",
       "      <td>0.966072</td>\n",
       "      <td>0.859903</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.672694</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>24.404643</td>\n",
       "      <td>0.25</td>\n",
       "      <td>False</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.943457</td>\n",
       "      <td>0.787275</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.577656</td>\n",
       "      <td>0.957648</td>\n",
       "      <td>0.847134</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.653371</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>35.635757</td>\n",
       "      <td>0.00</td>\n",
       "      <td>True</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.942964</td>\n",
       "      <td>0.788931</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.574669</td>\n",
       "      <td>0.973811</td>\n",
       "      <td>0.876582</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.693179</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>17.244989</td>\n",
       "      <td>0.00</td>\n",
       "      <td>True</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.942841</td>\n",
       "      <td>0.799037</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.569696</td>\n",
       "      <td>0.954603</td>\n",
       "      <td>0.851330</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.637638</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>15.595821</td>\n",
       "      <td>0.00</td>\n",
       "      <td>False</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.942604</td>\n",
       "      <td>0.782513</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.571389</td>\n",
       "      <td>0.954316</td>\n",
       "      <td>0.831461</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.639466</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>17.243771</td>\n",
       "      <td>0.25</td>\n",
       "      <td>True</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.943885</td>\n",
       "      <td>0.772311</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.577048</td>\n",
       "      <td>0.950923</td>\n",
       "      <td>0.811980</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.637638</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>21.756550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.943979</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.577417</td>\n",
       "      <td>0.971031</td>\n",
       "      <td>0.874598</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.688316</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>22.730584</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.943646</td>\n",
       "      <td>0.789589</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.576350</td>\n",
       "      <td>0.959904</td>\n",
       "      <td>0.856240</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.655590</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>23.225753</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.943822</td>\n",
       "      <td>0.786249</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.577494</td>\n",
       "      <td>0.959740</td>\n",
       "      <td>0.841935</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.658903</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>18.643156</td>\n",
       "      <td>0.25</td>\n",
       "      <td>False</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.943186</td>\n",
       "      <td>0.781686</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.578610</td>\n",
       "      <td>0.957635</td>\n",
       "      <td>0.836834</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.654901</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>17.858252</td>\n",
       "      <td>0.00</td>\n",
       "      <td>False</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.943124</td>\n",
       "      <td>0.788540</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.572255</td>\n",
       "      <td>0.961214</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.656655</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>21.158664</td>\n",
       "      <td>0.00</td>\n",
       "      <td>True</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.942119</td>\n",
       "      <td>0.788258</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.569514</td>\n",
       "      <td>0.954754</td>\n",
       "      <td>0.849921</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.639397</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>11.610081</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.944078</td>\n",
       "      <td>0.787140</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.574514</td>\n",
       "      <td>0.954466</td>\n",
       "      <td>0.830065</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.640702</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>10.541966</td>\n",
       "      <td>0.00</td>\n",
       "      <td>True</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.942815</td>\n",
       "      <td>0.787551</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.571816</td>\n",
       "      <td>0.963328</td>\n",
       "      <td>0.861635</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.663012</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>14.594290</td>\n",
       "      <td>0.25</td>\n",
       "      <td>True</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.941671</td>\n",
       "      <td>0.770757</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.573282</td>\n",
       "      <td>0.951960</td>\n",
       "      <td>0.822951</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.636904</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>12.665122</td>\n",
       "      <td>0.00</td>\n",
       "      <td>False</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.943180</td>\n",
       "      <td>0.790848</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.572979</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.851794</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.652412</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>21.511623</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.944304</td>\n",
       "      <td>0.781513</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.576336</td>\n",
       "      <td>0.952987</td>\n",
       "      <td>0.819079</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.641041</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>16.110693</td>\n",
       "      <td>0.25</td>\n",
       "      <td>False</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.943654</td>\n",
       "      <td>0.778121</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.576103</td>\n",
       "      <td>0.951982</td>\n",
       "      <td>0.830870</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.637891</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>17.157437</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.943861</td>\n",
       "      <td>0.777872</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.576006</td>\n",
       "      <td>0.953306</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.639952</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>16.604685</td>\n",
       "      <td>0.25</td>\n",
       "      <td>False</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.944098</td>\n",
       "      <td>0.762815</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.577707</td>\n",
       "      <td>0.951763</td>\n",
       "      <td>0.802030</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.640409</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>16.000157</td>\n",
       "      <td>0.25</td>\n",
       "      <td>True</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.942766</td>\n",
       "      <td>0.794198</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.576892</td>\n",
       "      <td>0.958877</td>\n",
       "      <td>0.859843</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.653799</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>13.908582</td>\n",
       "      <td>0.00</td>\n",
       "      <td>False</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.942799</td>\n",
       "      <td>0.787384</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.571412</td>\n",
       "      <td>0.953973</td>\n",
       "      <td>0.843949</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.639258</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>15.309449</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.944970</td>\n",
       "      <td>0.798370</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.579429</td>\n",
       "      <td>0.960479</td>\n",
       "      <td>0.857595</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.657006</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>2.327924</td>\n",
       "      <td>0.00</td>\n",
       "      <td>False</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.594618</td>\n",
       "      <td>0.188964</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.112280</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.988826</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>3.931227</td>\n",
       "      <td>0.00</td>\n",
       "      <td>True</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.765689</td>\n",
       "      <td>0.103943</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.329744</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.943880</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>11.402118</td>\n",
       "      <td>0.25</td>\n",
       "      <td>True</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.756063</td>\n",
       "      <td>0.011407</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.324444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.943880</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>2.370452</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.587277</td>\n",
       "      <td>0.045866</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.113663</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.943880</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>2.424492</td>\n",
       "      <td>0.25</td>\n",
       "      <td>False</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.494048</td>\n",
       "      <td>0.005669</td>\n",
       "      <td>9492</td>\n",
       "      <td>-0.002818</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.988826</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>4.006525</td>\n",
       "      <td>0.25</td>\n",
       "      <td>True</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.783036</td>\n",
       "      <td>0.013920</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.357940</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.943880</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>1.335362</td>\n",
       "      <td>0.00</td>\n",
       "      <td>False</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.534657</td>\n",
       "      <td>0.223532</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.035464</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.988826</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>2.225411</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.803651</td>\n",
       "      <td>0.046613</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.378313</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.943880</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>1.339984</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.612577</td>\n",
       "      <td>0.068067</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.129387</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.943880</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>6.957234</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.750401</td>\n",
       "      <td>0.008254</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.321398</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.943880</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>3.134966</td>\n",
       "      <td>0.25</td>\n",
       "      <td>True</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.741444</td>\n",
       "      <td>0.005729</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.309508</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.943880</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>1.466699</td>\n",
       "      <td>0.25</td>\n",
       "      <td>False</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.633838</td>\n",
       "      <td>0.003823</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.176398</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.988826</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>4.257955</td>\n",
       "      <td>0.25</td>\n",
       "      <td>False</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.897696</td>\n",
       "      <td>0.639138</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.520376</td>\n",
       "      <td>0.936073</td>\n",
       "      <td>0.736000</td>\n",
       "      <td>250</td>\n",
       "      <td>0.647527</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>3.310624</td>\n",
       "      <td>0.25</td>\n",
       "      <td>False</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.897738</td>\n",
       "      <td>0.648458</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.521534</td>\n",
       "      <td>0.931352</td>\n",
       "      <td>0.740157</td>\n",
       "      <td>250</td>\n",
       "      <td>0.641327</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>9.144688</td>\n",
       "      <td>0.25</td>\n",
       "      <td>True</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.890707</td>\n",
       "      <td>0.581456</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.511921</td>\n",
       "      <td>0.944199</td>\n",
       "      <td>0.710744</td>\n",
       "      <td>250</td>\n",
       "      <td>0.677353</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>8.642633</td>\n",
       "      <td>0.25</td>\n",
       "      <td>True</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.896314</td>\n",
       "      <td>0.661006</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.517552</td>\n",
       "      <td>0.936073</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>250</td>\n",
       "      <td>0.647788</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>3.206089</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.895925</td>\n",
       "      <td>0.668093</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.517234</td>\n",
       "      <td>0.944432</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>250</td>\n",
       "      <td>0.657643</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>4.180880</td>\n",
       "      <td>0.25</td>\n",
       "      <td>True</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.897942</td>\n",
       "      <td>0.659830</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.520530</td>\n",
       "      <td>0.929959</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>250</td>\n",
       "      <td>0.642175</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>8.667757</td>\n",
       "      <td>0.00</td>\n",
       "      <td>True</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.888412</td>\n",
       "      <td>0.674719</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.504174</td>\n",
       "      <td>0.977092</td>\n",
       "      <td>0.832117</td>\n",
       "      <td>250</td>\n",
       "      <td>0.727477</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>6.468171</td>\n",
       "      <td>0.00</td>\n",
       "      <td>True</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.887690</td>\n",
       "      <td>0.671464</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.500818</td>\n",
       "      <td>0.956041</td>\n",
       "      <td>0.796992</td>\n",
       "      <td>250</td>\n",
       "      <td>0.667955</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>6.461937</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.897568</td>\n",
       "      <td>0.668663</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.518950</td>\n",
       "      <td>0.947527</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>250</td>\n",
       "      <td>0.662864</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>3.040146</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.900622</td>\n",
       "      <td>0.664944</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.525219</td>\n",
       "      <td>0.932126</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>250</td>\n",
       "      <td>0.640544</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>3.419461</td>\n",
       "      <td>0.00</td>\n",
       "      <td>False</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.881871</td>\n",
       "      <td>0.664188</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.490281</td>\n",
       "      <td>0.948456</td>\n",
       "      <td>0.784615</td>\n",
       "      <td>250</td>\n",
       "      <td>0.641588</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>9.798665</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.899440</td>\n",
       "      <td>0.684177</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.520419</td>\n",
       "      <td>0.934912</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>250</td>\n",
       "      <td>0.651443</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>3.177258</td>\n",
       "      <td>0.00</td>\n",
       "      <td>False</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.883099</td>\n",
       "      <td>0.669142</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.494211</td>\n",
       "      <td>0.950004</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>250</td>\n",
       "      <td>0.657708</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>5.231812</td>\n",
       "      <td>0.25</td>\n",
       "      <td>False</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.889365</td>\n",
       "      <td>0.587074</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.510096</td>\n",
       "      <td>0.937621</td>\n",
       "      <td>0.694215</td>\n",
       "      <td>250</td>\n",
       "      <td>0.649159</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>3.053943</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.893986</td>\n",
       "      <td>0.646113</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.512854</td>\n",
       "      <td>0.935918</td>\n",
       "      <td>0.775194</td>\n",
       "      <td>250</td>\n",
       "      <td>0.649746</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>4.423149</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.896100</td>\n",
       "      <td>0.664364</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.517786</td>\n",
       "      <td>0.937776</td>\n",
       "      <td>0.746032</td>\n",
       "      <td>250</td>\n",
       "      <td>0.649224</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>8.691315</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.892821</td>\n",
       "      <td>0.659076</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.511016</td>\n",
       "      <td>0.974073</td>\n",
       "      <td>0.820896</td>\n",
       "      <td>250</td>\n",
       "      <td>0.724670</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>3.914488</td>\n",
       "      <td>0.00</td>\n",
       "      <td>True</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.892428</td>\n",
       "      <td>0.677861</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.508225</td>\n",
       "      <td>0.943193</td>\n",
       "      <td>0.759690</td>\n",
       "      <td>250</td>\n",
       "      <td>0.651639</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>3.444990</td>\n",
       "      <td>0.00</td>\n",
       "      <td>False</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.883848</td>\n",
       "      <td>0.665980</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.494038</td>\n",
       "      <td>0.947372</td>\n",
       "      <td>0.781955</td>\n",
       "      <td>250</td>\n",
       "      <td>0.648375</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>3.698590</td>\n",
       "      <td>0.25</td>\n",
       "      <td>False</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.892111</td>\n",
       "      <td>0.590866</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.511988</td>\n",
       "      <td>0.929030</td>\n",
       "      <td>0.710744</td>\n",
       "      <td>250</td>\n",
       "      <td>0.632842</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>9.661267</td>\n",
       "      <td>0.00</td>\n",
       "      <td>True</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.888536</td>\n",
       "      <td>0.683283</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.502111</td>\n",
       "      <td>0.951397</td>\n",
       "      <td>0.766917</td>\n",
       "      <td>250</td>\n",
       "      <td>0.649942</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>4.416621</td>\n",
       "      <td>0.25</td>\n",
       "      <td>True</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.899072</td>\n",
       "      <td>0.644828</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.522444</td>\n",
       "      <td>0.935067</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>250</td>\n",
       "      <td>0.656338</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>5.097387</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.897612</td>\n",
       "      <td>0.674586</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.518353</td>\n",
       "      <td>0.946289</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>250</td>\n",
       "      <td>0.658948</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>5.193939</td>\n",
       "      <td>0.00</td>\n",
       "      <td>False</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'numpy.exp(-epoch)', u'im...</td>\n",
       "      <td>numpy.exp(-epoch)</td>\n",
       "      <td>0.890115</td>\n",
       "      <td>0.669293</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.503671</td>\n",
       "      <td>0.949230</td>\n",
       "      <td>0.763359</td>\n",
       "      <td>250</td>\n",
       "      <td>0.654510</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   activation          allele  dropout_probability  embedding_output_dim  \\\n",
       "0        tanh  HLA-A0201-1000                  0.5                    32   \n",
       "1        tanh  HLA-A0201-1000                  0.5                    32   \n",
       "2        tanh  HLA-A0201-1000                  0.5                    32   \n",
       "3        tanh  HLA-A0201-1000                  0.5                    32   \n",
       "4        tanh  HLA-A0201-1000                  0.5                    32   \n",
       "5        tanh  HLA-A0201-1000                  0.5                    32   \n",
       "6        tanh  HLA-A0201-1000                  0.5                    32   \n",
       "7        tanh  HLA-A0201-1000                  0.5                    32   \n",
       "8        tanh  HLA-A0201-1000                  0.5                    32   \n",
       "9        tanh  HLA-A0201-1000                  0.5                    32   \n",
       "10       tanh  HLA-A0201-1000                  0.5                    32   \n",
       "11       tanh  HLA-A0201-1000                  0.5                    32   \n",
       "12       tanh  HLA-A0201-1000                  0.5                    32   \n",
       "13       tanh  HLA-A0201-1000                  0.5                    32   \n",
       "14       tanh  HLA-A0201-1000                  0.5                    32   \n",
       "15       tanh  HLA-A0201-1000                  0.5                    32   \n",
       "16       tanh  HLA-A0201-1000                  0.5                    32   \n",
       "17       tanh  HLA-A0201-1000                  0.5                    32   \n",
       "18       tanh  HLA-A0201-1000                  0.5                    32   \n",
       "19       tanh  HLA-A0201-1000                  0.5                    32   \n",
       "20       tanh  HLA-A0201-1000                  0.5                    32   \n",
       "21       tanh  HLA-A0201-1000                  0.5                    32   \n",
       "22       tanh  HLA-A0201-1000                  0.5                    32   \n",
       "23       tanh  HLA-A0201-1000                  0.5                    32   \n",
       "24       tanh    HLA-A0201-10                  0.5                    32   \n",
       "25       tanh    HLA-A0201-10                  0.5                    32   \n",
       "26       tanh    HLA-A0201-10                  0.5                    32   \n",
       "27       tanh    HLA-A0201-10                  0.5                    32   \n",
       "28       tanh    HLA-A0201-10                  0.5                    32   \n",
       "29       tanh    HLA-A0201-10                  0.5                    32   \n",
       "..        ...             ...                  ...                   ...   \n",
       "42       tanh    HLA-A0201-10                  0.5                    32   \n",
       "43       tanh    HLA-A0201-10                  0.5                    32   \n",
       "44       tanh    HLA-A0201-10                  0.5                    32   \n",
       "45       tanh    HLA-A0201-10                  0.5                    32   \n",
       "46       tanh    HLA-A0201-10                  0.5                    32   \n",
       "47       tanh    HLA-A0201-10                  0.5                    32   \n",
       "48       tanh   HLA-A0201-250                  0.5                    32   \n",
       "49       tanh   HLA-A0201-250                  0.5                    32   \n",
       "50       tanh   HLA-A0201-250                  0.5                    32   \n",
       "51       tanh   HLA-A0201-250                  0.5                    32   \n",
       "52       tanh   HLA-A0201-250                  0.5                    32   \n",
       "53       tanh   HLA-A0201-250                  0.5                    32   \n",
       "54       tanh   HLA-A0201-250                  0.5                    32   \n",
       "55       tanh   HLA-A0201-250                  0.5                    32   \n",
       "56       tanh   HLA-A0201-250                  0.5                    32   \n",
       "57       tanh   HLA-A0201-250                  0.5                    32   \n",
       "58       tanh   HLA-A0201-250                  0.5                    32   \n",
       "59       tanh   HLA-A0201-250                  0.5                    32   \n",
       "60       tanh   HLA-A0201-250                  0.5                    32   \n",
       "61       tanh   HLA-A0201-250                  0.5                    32   \n",
       "62       tanh   HLA-A0201-250                  0.5                    32   \n",
       "63       tanh   HLA-A0201-250                  0.5                    32   \n",
       "64       tanh   HLA-A0201-250                  0.5                    32   \n",
       "65       tanh   HLA-A0201-250                  0.5                    32   \n",
       "66       tanh   HLA-A0201-250                  0.5                    32   \n",
       "67       tanh   HLA-A0201-250                  0.5                    32   \n",
       "68       tanh   HLA-A0201-250                  0.5                    32   \n",
       "69       tanh   HLA-A0201-250                  0.5                    32   \n",
       "70       tanh   HLA-A0201-250                  0.5                    32   \n",
       "71       tanh   HLA-A0201-250                  0.5                    32   \n",
       "\n",
       "     fit_time  fraction_negative impute layer_sizes  \\\n",
       "0    9.020235               0.10  False         [4]   \n",
       "1   21.483086               0.25   True       [128]   \n",
       "2   24.404643               0.25  False       [128]   \n",
       "3   35.635757               0.00   True       [128]   \n",
       "4   17.244989               0.00   True         [4]   \n",
       "5   15.595821               0.00  False         [4]   \n",
       "6   17.243771               0.25   True         [4]   \n",
       "7   21.756550               0.10   True       [128]   \n",
       "8   22.730584               0.10  False       [128]   \n",
       "9   23.225753               0.10  False       [128]   \n",
       "10  18.643156               0.25  False       [128]   \n",
       "11  17.858252               0.00  False       [128]   \n",
       "12  21.158664               0.00   True         [4]   \n",
       "13  11.610081               0.10  False         [4]   \n",
       "14  10.541966               0.00   True       [128]   \n",
       "15  14.594290               0.25   True         [4]   \n",
       "16  12.665122               0.00  False       [128]   \n",
       "17  21.511623               0.10   True         [4]   \n",
       "18  16.110693               0.25  False         [4]   \n",
       "19  17.157437               0.10   True         [4]   \n",
       "20  16.604685               0.25  False         [4]   \n",
       "21  16.000157               0.25   True       [128]   \n",
       "22  13.908582               0.00  False         [4]   \n",
       "23  15.309449               0.10   True       [128]   \n",
       "24   2.327924               0.00  False         [4]   \n",
       "25   3.931227               0.00   True         [4]   \n",
       "26  11.402118               0.25   True         [4]   \n",
       "27   2.370452               0.10  False         [4]   \n",
       "28   2.424492               0.25  False         [4]   \n",
       "29   4.006525               0.25   True         [4]   \n",
       "..        ...                ...    ...         ...   \n",
       "42   1.335362               0.00  False       [128]   \n",
       "43   2.225411               0.10   True         [4]   \n",
       "44   1.339984               0.10  False         [4]   \n",
       "45   6.957234               0.10   True       [128]   \n",
       "46   3.134966               0.25   True       [128]   \n",
       "47   1.466699               0.25  False       [128]   \n",
       "48   4.257955               0.25  False       [128]   \n",
       "49   3.310624               0.25  False         [4]   \n",
       "50   9.144688               0.25   True       [128]   \n",
       "51   8.642633               0.25   True         [4]   \n",
       "52   3.206089               0.10  False       [128]   \n",
       "53   4.180880               0.25   True         [4]   \n",
       "54   8.667757               0.00   True       [128]   \n",
       "55   6.468171               0.00   True       [128]   \n",
       "56   6.461937               0.10   True       [128]   \n",
       "57   3.040146               0.10  False         [4]   \n",
       "58   3.419461               0.00  False         [4]   \n",
       "59   9.798665               0.10   True         [4]   \n",
       "60   3.177258               0.00  False       [128]   \n",
       "61   5.231812               0.25  False       [128]   \n",
       "62   3.053943               0.10  False         [4]   \n",
       "63   4.423149               0.10   True         [4]   \n",
       "64   8.691315               0.10   True       [128]   \n",
       "65   3.914488               0.00   True         [4]   \n",
       "66   3.444990               0.00  False         [4]   \n",
       "67   3.698590               0.25  False         [4]   \n",
       "68   9.661267               0.00   True         [4]   \n",
       "69   4.416621               0.25   True       [128]   \n",
       "70   5.097387               0.10  False       [128]   \n",
       "71   5.193939               0.00  False       [128]   \n",
       "\n",
       "                                         model_params     pretrain_decay  \\\n",
       "0   {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "1   {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "2   {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "3   {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "4   {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "5   {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "6   {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "7   {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "8   {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "9   {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "10  {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "11  {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "12  {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "13  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "14  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "15  {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "16  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "17  {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "18  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "19  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "20  {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "21  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "22  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "23  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "24  {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "25  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "26  {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "27  {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "28  {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "29  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "..                                                ...                ...   \n",
       "42  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "43  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "44  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "45  {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "46  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "47  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "48  {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "49  {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "50  {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "51  {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "52  {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "53  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "54  {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "55  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "56  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "57  {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "58  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "59  {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "60  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "61  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "62  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "63  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "64  {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "65  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "66  {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "67  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "68  {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "69  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "70  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...   1 / (1+epoch)**2   \n",
       "71  {u'pretrain_decay': u'numpy.exp(-epoch)', u'im...  numpy.exp(-epoch)   \n",
       "\n",
       "    test_auc   test_f1  test_size  test_tau  train_auc  train_f1  train_size  \\\n",
       "0   0.944129  0.781538       6532  0.575135   0.954311  0.826797        1000   \n",
       "1   0.944119  0.788403       6532  0.579707   0.966072  0.859903        1000   \n",
       "2   0.943457  0.787275       6532  0.577656   0.957648  0.847134        1000   \n",
       "3   0.942964  0.788931       6532  0.574669   0.973811  0.876582        1000   \n",
       "4   0.942841  0.799037       6532  0.569696   0.954603  0.851330        1000   \n",
       "5   0.942604  0.782513       6532  0.571389   0.954316  0.831461        1000   \n",
       "6   0.943885  0.772311       6532  0.577048   0.950923  0.811980        1000   \n",
       "7   0.943979  0.785000       6532  0.577417   0.971031  0.874598        1000   \n",
       "8   0.943646  0.789589       6532  0.576350   0.959904  0.856240        1000   \n",
       "9   0.943822  0.786249       6532  0.577494   0.959740  0.841935        1000   \n",
       "10  0.943186  0.781686       6532  0.578610   0.957635  0.836834        1000   \n",
       "11  0.943124  0.788540       6532  0.572255   0.961214  0.857143        1000   \n",
       "12  0.942119  0.788258       6532  0.569514   0.954754  0.849921        1000   \n",
       "13  0.944078  0.787140       6532  0.574514   0.954466  0.830065        1000   \n",
       "14  0.942815  0.787551       6532  0.571816   0.963328  0.861635        1000   \n",
       "15  0.941671  0.770757       6532  0.573282   0.951960  0.822951        1000   \n",
       "16  0.943180  0.790848       6532  0.572979   0.958333  0.851794        1000   \n",
       "17  0.944304  0.781513       6532  0.576336   0.952987  0.819079        1000   \n",
       "18  0.943654  0.778121       6532  0.576103   0.951982  0.830870        1000   \n",
       "19  0.943861  0.777872       6532  0.576006   0.953306  0.819672        1000   \n",
       "20  0.944098  0.762815       6532  0.577707   0.951763  0.802030        1000   \n",
       "21  0.942766  0.794198       6532  0.576892   0.958877  0.859843        1000   \n",
       "22  0.942799  0.787384       6532  0.571412   0.953973  0.843949        1000   \n",
       "23  0.944970  0.798370       6532  0.579429   0.960479  0.857595        1000   \n",
       "24  0.594618  0.188964       9492  0.112280   1.000000  0.800000          10   \n",
       "25  0.765689  0.103943       9492  0.329744   1.000000  0.800000          10   \n",
       "26  0.756063  0.011407       9492  0.324444   1.000000  0.800000          10   \n",
       "27  0.587277  0.045866       9492  0.113663   1.000000  0.800000          10   \n",
       "28  0.494048  0.005669       9492 -0.002818   1.000000  0.800000          10   \n",
       "29  0.783036  0.013920       9492  0.357940   1.000000  0.800000          10   \n",
       "..       ...       ...        ...       ...        ...       ...         ...   \n",
       "42  0.534657  0.223532       9492  0.035464   1.000000  1.000000          10   \n",
       "43  0.803651  0.046613       9492  0.378313   1.000000  0.800000          10   \n",
       "44  0.612577  0.068067       9492  0.129387   1.000000  0.800000          10   \n",
       "45  0.750401  0.008254       9492  0.321398   1.000000  1.000000          10   \n",
       "46  0.741444  0.005729       9492  0.309508   1.000000  1.000000          10   \n",
       "47  0.633838  0.003823       9492  0.176398   1.000000  0.800000          10   \n",
       "48  0.897696  0.639138       8482  0.520376   0.936073  0.736000         250   \n",
       "49  0.897738  0.648458       8482  0.521534   0.931352  0.740157         250   \n",
       "50  0.890707  0.581456       8482  0.511921   0.944199  0.710744         250   \n",
       "51  0.896314  0.661006       8482  0.517552   0.936073  0.753846         250   \n",
       "52  0.895925  0.668093       8482  0.517234   0.944432  0.787879         250   \n",
       "53  0.897942  0.659830       8482  0.520530   0.929959  0.730159         250   \n",
       "54  0.888412  0.674719       8482  0.504174   0.977092  0.832117         250   \n",
       "55  0.887690  0.671464       8482  0.500818   0.956041  0.796992         250   \n",
       "56  0.897568  0.668663       8482  0.518950   0.947527  0.750000         250   \n",
       "57  0.900622  0.664944       8482  0.525219   0.932126  0.744186         250   \n",
       "58  0.881871  0.664188       8482  0.490281   0.948456  0.784615         250   \n",
       "59  0.899440  0.684177       8482  0.520419   0.934912  0.757576         250   \n",
       "60  0.883099  0.669142       8482  0.494211   0.950004  0.757576         250   \n",
       "61  0.889365  0.587074       8482  0.510096   0.937621  0.694215         250   \n",
       "62  0.893986  0.646113       8482  0.512854   0.935918  0.775194         250   \n",
       "63  0.896100  0.664364       8482  0.517786   0.937776  0.746032         250   \n",
       "64  0.892821  0.659076       8482  0.511016   0.974073  0.820896         250   \n",
       "65  0.892428  0.677861       8482  0.508225   0.943193  0.759690         250   \n",
       "66  0.883848  0.665980       8482  0.494038   0.947372  0.781955         250   \n",
       "67  0.892111  0.590866       8482  0.511988   0.929030  0.710744         250   \n",
       "68  0.888536  0.683283       8482  0.502111   0.951397  0.766917         250   \n",
       "69  0.899072  0.644828       8482  0.522444   0.935067  0.704000         250   \n",
       "70  0.897612  0.674586       8482  0.518353   0.946289  0.769231         250   \n",
       "71  0.890115  0.669293       8482  0.503671   0.949230  0.763359         250   \n",
       "\n",
       "    train_tau  layer0_size  \n",
       "0    0.639283            4  \n",
       "1    0.672694          128  \n",
       "2    0.653371          128  \n",
       "3    0.693179          128  \n",
       "4    0.637638            4  \n",
       "5    0.639466            4  \n",
       "6    0.637638            4  \n",
       "7    0.688316          128  \n",
       "8    0.655590          128  \n",
       "9    0.658903          128  \n",
       "10   0.654901          128  \n",
       "11   0.656655          128  \n",
       "12   0.639397            4  \n",
       "13   0.640702            4  \n",
       "14   0.663012          128  \n",
       "15   0.636904            4  \n",
       "16   0.652412          128  \n",
       "17   0.641041            4  \n",
       "18   0.637891            4  \n",
       "19   0.639952            4  \n",
       "20   0.640409            4  \n",
       "21   0.653799          128  \n",
       "22   0.639258            4  \n",
       "23   0.657006          128  \n",
       "24   0.988826            4  \n",
       "25   0.943880            4  \n",
       "26   0.943880            4  \n",
       "27   0.943880            4  \n",
       "28   0.988826            4  \n",
       "29   0.943880            4  \n",
       "..        ...          ...  \n",
       "42   0.988826          128  \n",
       "43   0.943880            4  \n",
       "44   0.943880            4  \n",
       "45   0.943880          128  \n",
       "46   0.943880          128  \n",
       "47   0.988826          128  \n",
       "48   0.647527          128  \n",
       "49   0.641327            4  \n",
       "50   0.677353          128  \n",
       "51   0.647788            4  \n",
       "52   0.657643          128  \n",
       "53   0.642175            4  \n",
       "54   0.727477          128  \n",
       "55   0.667955          128  \n",
       "56   0.662864          128  \n",
       "57   0.640544            4  \n",
       "58   0.641588            4  \n",
       "59   0.651443            4  \n",
       "60   0.657708          128  \n",
       "61   0.649159          128  \n",
       "62   0.649746            4  \n",
       "63   0.649224            4  \n",
       "64   0.724670          128  \n",
       "65   0.651639            4  \n",
       "66   0.648375            4  \n",
       "67   0.632842            4  \n",
       "68   0.649942            4  \n",
       "69   0.656338          128  \n",
       "70   0.658948          128  \n",
       "71   0.654510          128  \n",
       "\n",
       "[72 rows x 19 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df = pandas.DataFrame(cv_df)\n",
    "cv_df[\"layer0_size\"] = [x[0] for x in cv_df.layer_sizes]\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/anaconda2/envs/standard-2.7/lib/python2.7/site-packages/ipykernel/__main__.py:2: FutureWarning: sort is deprecated, use sort_values(inplace=True) for INPLACE sorting\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>test_auc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_string</th>\n",
       "      <th>train_size</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>1000</th>\n",
       "      <td>0.944920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>1000</th>\n",
       "      <td>0.944612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [256], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>1000</th>\n",
       "      <td>0.944024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [4], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>1000</th>\n",
       "      <td>0.943817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [256], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>1000</th>\n",
       "      <td>0.943776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [4], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>1000</th>\n",
       "      <td>0.943569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [128], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>1000</th>\n",
       "      <td>0.943261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [128], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>1000</th>\n",
       "      <td>0.943004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [4], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>1000</th>\n",
       "      <td>0.924926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [256], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>1000</th>\n",
       "      <td>0.924763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [128], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>1000</th>\n",
       "      <td>0.924209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [64], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>1000</th>\n",
       "      <td>0.923279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [256], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>1000</th>\n",
       "      <td>0.922751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [4], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>1000</th>\n",
       "      <td>0.922142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [64], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>1000</th>\n",
       "      <td>0.921942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [128], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>1000</th>\n",
       "      <td>0.921489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [128], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>250</th>\n",
       "      <td>0.895955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [256], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>250</th>\n",
       "      <td>0.895112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [128], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>250</th>\n",
       "      <td>0.895087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [4], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>250</th>\n",
       "      <td>0.894884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>250</th>\n",
       "      <td>0.893989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [256], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>250</th>\n",
       "      <td>0.893219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [256], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>250</th>\n",
       "      <td>0.893073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [128], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>250</th>\n",
       "      <td>0.892656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [4], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>250</th>\n",
       "      <td>0.891843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>250</th>\n",
       "      <td>0.889847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [4], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>250</th>\n",
       "      <td>0.885435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [64], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>250</th>\n",
       "      <td>0.884448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [64], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>250</th>\n",
       "      <td>0.883239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [128], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>250</th>\n",
       "      <td>0.879062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [256], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>250</th>\n",
       "      <td>0.878753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [4], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>250</th>\n",
       "      <td>0.875744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [4], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>10</th>\n",
       "      <td>0.788338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [256], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>10</th>\n",
       "      <td>0.778861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>10</th>\n",
       "      <td>0.766717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [128], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>10</th>\n",
       "      <td>0.720564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [128], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>10</th>\n",
       "      <td>0.648763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [4], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>10</th>\n",
       "      <td>0.647545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [64], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>10</th>\n",
       "      <td>0.636350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [128], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>10</th>\n",
       "      <td>0.631081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [256], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>10</th>\n",
       "      <td>0.629064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [256], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>10</th>\n",
       "      <td>0.607756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [4], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>10</th>\n",
       "      <td>0.587999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>10</th>\n",
       "      <td>0.584683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [256], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>10</th>\n",
       "      <td>0.537523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [4], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>10</th>\n",
       "      <td>0.522576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [128], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>10</th>\n",
       "      <td>0.518687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [64], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <th>10</th>\n",
       "      <td>0.502695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               test_auc\n",
       "model_string                                       train_size          \n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 1000        0.944920\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 1000        0.944612\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 1000        0.944024\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 1000        0.943817\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 1000        0.943776\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 1000        0.943569\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 1000        0.943261\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 1000        0.943004\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 1000        0.924926\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 1000        0.924763\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 1000        0.924209\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 1000        0.923279\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 1000        0.922751\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 1000        0.922142\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 1000        0.921942\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 1000        0.921489\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 250         0.895955\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 250         0.895112\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 250         0.895087\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 250         0.894884\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 250         0.893989\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 250         0.893219\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 250         0.893073\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 250         0.892656\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 250         0.891843\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 250         0.889847\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 250         0.885435\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 250         0.884448\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 250         0.883239\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 250         0.879062\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 250         0.878753\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 250         0.875744\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 10          0.788338\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 10          0.778861\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 10          0.766717\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 10          0.720564\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 10          0.648763\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 10          0.647545\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 10          0.636350\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 10          0.631081\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 10          0.629064\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 10          0.607756\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 10          0.587999\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 10          0.584683\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 10          0.537523\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 10          0.522576\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 10          0.518687\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'... 10          0.502695"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df[\"model_string\"] = [str(x) for x in cv_df.model_params]\n",
    "cv_df.groupby([\"model_string\", \"train_size\"]).test_auc.mean().sort(inplace=False, ascending=False).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/anaconda2/envs/standard-2.7/lib/python2.7/site-packages/ipykernel/__main__.py:1: FutureWarning: sort is deprecated, use sort_values(inplace=True) for INPLACE sorting\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_auc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_string</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [4], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <td>0.875597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [256], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <td>0.872666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <td>0.867162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [128], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <td>0.852075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [128], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <td>0.829037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [128], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <td>0.817082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [4], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <td>0.815144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [256], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <td>0.814963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [256], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <td>0.814917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [64], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <td>0.814289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <td>0.807761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [4], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <td>0.799453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [4], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <td>0.786079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [256], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <td>0.780346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [128], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <td>0.773079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'pretrain_decay': '1 / (1+epoch)**2', 'impute': False, 'dropout_probability': 0.5, 'embedding_output_dim': 4, 'layer_sizes': [64], 'fraction_negative': 0.1, 'activation': 'tanh'}</th>\n",
       "      <td>0.769695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    test_auc\n",
       "model_string                                                \n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'...  0.875597\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'...  0.872666\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'...  0.867162\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'...  0.852075\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'...  0.829037\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'...  0.817082\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'...  0.815144\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'...  0.814963\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'...  0.814917\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'...  0.814289\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'...  0.807761\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'...  0.799453\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'...  0.786079\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'...  0.780346\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'...  0.773079\n",
       "{'pretrain_decay': '1 / (1+epoch)**2', 'impute'...  0.769695"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df.groupby([\"model_string\"]).test_auc.mean().sort(inplace=False, ascending=False).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa4629a6dd0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAESCAYAAAAbq2nJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIcwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa4629a6710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seaborn.violinplot(data=cv_df, x=\"train_size\", y=\"test_auc\", hue=\"impute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_size  layer0_size\n",
       "10          4              0.788338\n",
       "            64             0.766717\n",
       "            128            0.720564\n",
       "            256            0.778861\n",
       "250         4              0.894884\n",
       "            64             0.893989\n",
       "            128            0.895955\n",
       "            256            0.895112\n",
       "1000        4              0.943817\n",
       "            64             0.944920\n",
       "            128            0.943261\n",
       "            256            0.944024\n",
       "Name: test_auc, dtype: float64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df.groupby([\"train_size\", \"layer0_size\"]).test_auc.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_size  impute\n",
       "10          False     0.648763\n",
       "            True      0.788338\n",
       "250         False     0.895087\n",
       "            True      0.895955\n",
       "1000        False     0.944612\n",
       "            True      0.944920\n",
       "Name: test_auc, dtype: float64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df.groupby([\"train_size\", \"impute\"]).test_auc.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/anaconda2/envs/standard-2.7/lib/python2.7/site-packages/ipykernel/__main__.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>allele</th>\n",
       "      <th>dropout_probability</th>\n",
       "      <th>embedding_output_dim</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>fraction_negative</th>\n",
       "      <th>impute</th>\n",
       "      <th>layer_sizes</th>\n",
       "      <th>model_params</th>\n",
       "      <th>pretrain_decay</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_size</th>\n",
       "      <th>test_tau</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_size</th>\n",
       "      <th>train_tau</th>\n",
       "      <th>layer0_size</th>\n",
       "      <th>model_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>13.426505</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>[64]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.944920</td>\n",
       "      <td>0.792123</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.578521</td>\n",
       "      <td>0.958178</td>\n",
       "      <td>0.846645</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.649511</td>\n",
       "      <td>64</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>12.238065</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>[64]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.944612</td>\n",
       "      <td>0.789052</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.577878</td>\n",
       "      <td>0.957548</td>\n",
       "      <td>0.845161</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.648197</td>\n",
       "      <td>64</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>15.937529</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>[256]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.944024</td>\n",
       "      <td>0.774943</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.578387</td>\n",
       "      <td>0.963812</td>\n",
       "      <td>0.844517</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.667076</td>\n",
       "      <td>256</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>10.549995</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.943817</td>\n",
       "      <td>0.774866</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.574753</td>\n",
       "      <td>0.954256</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.640486</td>\n",
       "      <td>4</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>14.192804</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>[256]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.943776</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.575957</td>\n",
       "      <td>0.961420</td>\n",
       "      <td>0.860720</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.662286</td>\n",
       "      <td>256</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>11.623836</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.943569</td>\n",
       "      <td>0.792068</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.573797</td>\n",
       "      <td>0.953553</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.639674</td>\n",
       "      <td>4</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>13.778689</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.943261</td>\n",
       "      <td>0.788530</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.574798</td>\n",
       "      <td>0.959685</td>\n",
       "      <td>0.863492</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.654281</td>\n",
       "      <td>128</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>10.565188</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.943004</td>\n",
       "      <td>0.777246</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.574835</td>\n",
       "      <td>0.962954</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.663518</td>\n",
       "      <td>128</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>11.624597</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.924926</td>\n",
       "      <td>0.661178</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.550081</td>\n",
       "      <td>0.927693</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.595068</td>\n",
       "      <td>4</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>13.419750</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>[256]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.924763</td>\n",
       "      <td>0.677850</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.551431</td>\n",
       "      <td>0.926912</td>\n",
       "      <td>0.726950</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.593954</td>\n",
       "      <td>256</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>12.906013</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.924209</td>\n",
       "      <td>0.713571</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.549220</td>\n",
       "      <td>0.926638</td>\n",
       "      <td>0.752941</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.590873</td>\n",
       "      <td>128</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>9.648340</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>[64]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.923279</td>\n",
       "      <td>0.685073</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.545595</td>\n",
       "      <td>0.922228</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.585888</td>\n",
       "      <td>64</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>14.581950</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>[256]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.922751</td>\n",
       "      <td>0.684932</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.544787</td>\n",
       "      <td>0.922721</td>\n",
       "      <td>0.735552</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.587438</td>\n",
       "      <td>256</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>12.843067</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.922142</td>\n",
       "      <td>0.668957</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.543873</td>\n",
       "      <td>0.923145</td>\n",
       "      <td>0.720848</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.586308</td>\n",
       "      <td>4</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>8.698821</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>[64]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.921942</td>\n",
       "      <td>0.709810</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.546140</td>\n",
       "      <td>0.926168</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.588279</td>\n",
       "      <td>64</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>8.868767</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.921489</td>\n",
       "      <td>0.708394</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.545831</td>\n",
       "      <td>0.924899</td>\n",
       "      <td>0.765391</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>128</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>3.915362</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.895955</td>\n",
       "      <td>0.558736</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.517298</td>\n",
       "      <td>0.898150</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>250</td>\n",
       "      <td>0.589767</td>\n",
       "      <td>128</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>5.799231</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>[256]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.895112</td>\n",
       "      <td>0.677324</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.515190</td>\n",
       "      <td>0.953719</td>\n",
       "      <td>0.775194</td>\n",
       "      <td>250</td>\n",
       "      <td>0.683423</td>\n",
       "      <td>256</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>3.958769</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.895087</td>\n",
       "      <td>0.659772</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.515716</td>\n",
       "      <td>0.948301</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>250</td>\n",
       "      <td>0.659014</td>\n",
       "      <td>128</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>4.495596</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.894884</td>\n",
       "      <td>0.654356</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.515058</td>\n",
       "      <td>0.942651</td>\n",
       "      <td>0.755906</td>\n",
       "      <td>250</td>\n",
       "      <td>0.650333</td>\n",
       "      <td>4</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>3.002649</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>[64]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.893989</td>\n",
       "      <td>0.658652</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.514013</td>\n",
       "      <td>0.943658</td>\n",
       "      <td>0.766917</td>\n",
       "      <td>250</td>\n",
       "      <td>0.655685</td>\n",
       "      <td>64</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>4.275779</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>[256]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.893219</td>\n",
       "      <td>0.660043</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.514081</td>\n",
       "      <td>0.949694</td>\n",
       "      <td>0.763359</td>\n",
       "      <td>250</td>\n",
       "      <td>0.663060</td>\n",
       "      <td>256</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>4.928189</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>[256]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.893073</td>\n",
       "      <td>0.550761</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.512360</td>\n",
       "      <td>0.902097</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>250</td>\n",
       "      <td>0.587157</td>\n",
       "      <td>256</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>5.147194</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.892656</td>\n",
       "      <td>0.666525</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.511780</td>\n",
       "      <td>0.950159</td>\n",
       "      <td>0.778626</td>\n",
       "      <td>250</td>\n",
       "      <td>0.667041</td>\n",
       "      <td>128</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>3.453106</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.891843</td>\n",
       "      <td>0.640374</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.511244</td>\n",
       "      <td>0.933364</td>\n",
       "      <td>0.725806</td>\n",
       "      <td>250</td>\n",
       "      <td>0.649420</td>\n",
       "      <td>4</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>5.516232</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>[64]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.889847</td>\n",
       "      <td>0.666104</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.507754</td>\n",
       "      <td>0.947682</td>\n",
       "      <td>0.763359</td>\n",
       "      <td>250</td>\n",
       "      <td>0.665997</td>\n",
       "      <td>64</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>4.009234</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.885435</td>\n",
       "      <td>0.535777</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.503301</td>\n",
       "      <td>0.892888</td>\n",
       "      <td>0.523364</td>\n",
       "      <td>250</td>\n",
       "      <td>0.557592</td>\n",
       "      <td>4</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>3.019351</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>[64]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.884448</td>\n",
       "      <td>0.545229</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.497892</td>\n",
       "      <td>0.892114</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>250</td>\n",
       "      <td>0.568883</td>\n",
       "      <td>64</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>3.884387</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>[64]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.883239</td>\n",
       "      <td>0.541594</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.498539</td>\n",
       "      <td>0.893352</td>\n",
       "      <td>0.589286</td>\n",
       "      <td>250</td>\n",
       "      <td>0.565685</td>\n",
       "      <td>64</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>3.678478</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.879062</td>\n",
       "      <td>0.498576</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.491699</td>\n",
       "      <td>0.896989</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>250</td>\n",
       "      <td>0.576649</td>\n",
       "      <td>128</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>3.473869</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>[256]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.878753</td>\n",
       "      <td>0.495090</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.493345</td>\n",
       "      <td>0.885226</td>\n",
       "      <td>0.576577</td>\n",
       "      <td>250</td>\n",
       "      <td>0.569862</td>\n",
       "      <td>256</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>5.252091</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.875744</td>\n",
       "      <td>0.472901</td>\n",
       "      <td>8482</td>\n",
       "      <td>0.485739</td>\n",
       "      <td>0.885535</td>\n",
       "      <td>0.537037</td>\n",
       "      <td>250</td>\n",
       "      <td>0.545387</td>\n",
       "      <td>4</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>2.368640</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.788338</td>\n",
       "      <td>0.039888</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.364708</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.988826</td>\n",
       "      <td>4</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>2.609647</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>[256]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.778861</td>\n",
       "      <td>0.015808</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.353579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.943880</td>\n",
       "      <td>256</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>2.593981</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>[64]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.766717</td>\n",
       "      <td>0.016430</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.339995</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.943880</td>\n",
       "      <td>64</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>2.303217</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.720564</td>\n",
       "      <td>0.014539</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.281390</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.988826</td>\n",
       "      <td>128</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>1.378262</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.648763</td>\n",
       "      <td>0.020038</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.186176</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.943880</td>\n",
       "      <td>128</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>2.395534</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.647545</td>\n",
       "      <td>0.016970</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.191832</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.898933</td>\n",
       "      <td>4</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>2.333883</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>[64]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.636350</td>\n",
       "      <td>0.013253</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.173754</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.943880</td>\n",
       "      <td>64</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>2.406448</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.631081</td>\n",
       "      <td>0.047196</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.175516</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.943880</td>\n",
       "      <td>128</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>2.581795</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>[256]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.629064</td>\n",
       "      <td>0.039567</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.165813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.898933</td>\n",
       "      <td>256</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>1.392699</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>[256]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.607756</td>\n",
       "      <td>0.006349</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.145288</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.943880</td>\n",
       "      <td>256</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.511511</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.587999</td>\n",
       "      <td>0.178299</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.095901</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.764093</td>\n",
       "      <td>4</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>1.415610</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>[64]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.584683</td>\n",
       "      <td>0.025990</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.111075</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.943880</td>\n",
       "      <td>64</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.369998</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>[256]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.537523</td>\n",
       "      <td>0.035987</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.043781</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.898933</td>\n",
       "      <td>256</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>1.365929</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.522576</td>\n",
       "      <td>0.066493</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.028709</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.943880</td>\n",
       "      <td>4</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.506067</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.518687</td>\n",
       "      <td>0.035747</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.024902</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.943880</td>\n",
       "      <td>128</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201-10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.354459</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>[64]</td>\n",
       "      <td>{u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...</td>\n",
       "      <td>1 / (1+epoch)**2</td>\n",
       "      <td>0.502695</td>\n",
       "      <td>0.038531</td>\n",
       "      <td>9492</td>\n",
       "      <td>0.004088</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.674200</td>\n",
       "      <td>64</td>\n",
       "      <td>{'pretrain_decay': '1 / (1+epoch)**2', 'impute...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   activation          allele  dropout_probability  embedding_output_dim  \\\n",
       "14       tanh  HLA-A0201-1000                  0.5                    32   \n",
       "5        tanh  HLA-A0201-1000                  0.5                    32   \n",
       "6        tanh  HLA-A0201-1000                  0.5                    32   \n",
       "9        tanh  HLA-A0201-1000                  0.5                    32   \n",
       "13       tanh  HLA-A0201-1000                  0.5                    32   \n",
       "2        tanh  HLA-A0201-1000                  0.5                    32   \n",
       "7        tanh  HLA-A0201-1000                  0.5                    32   \n",
       "3        tanh  HLA-A0201-1000                  0.5                    32   \n",
       "8        tanh  HLA-A0201-1000                  0.5                     4   \n",
       "11       tanh  HLA-A0201-1000                  0.5                     4   \n",
       "4        tanh  HLA-A0201-1000                  0.5                     4   \n",
       "0        tanh  HLA-A0201-1000                  0.5                     4   \n",
       "12       tanh  HLA-A0201-1000                  0.5                     4   \n",
       "15       tanh  HLA-A0201-1000                  0.5                     4   \n",
       "1        tanh  HLA-A0201-1000                  0.5                     4   \n",
       "10       tanh  HLA-A0201-1000                  0.5                     4   \n",
       "41       tanh   HLA-A0201-250                  0.5                     4   \n",
       "32       tanh   HLA-A0201-250                  0.5                    32   \n",
       "34       tanh   HLA-A0201-250                  0.5                    32   \n",
       "40       tanh   HLA-A0201-250                  0.5                    32   \n",
       "37       tanh   HLA-A0201-250                  0.5                    32   \n",
       "33       tanh   HLA-A0201-250                  0.5                    32   \n",
       "38       tanh   HLA-A0201-250                  0.5                     4   \n",
       "47       tanh   HLA-A0201-250                  0.5                    32   \n",
       "43       tanh   HLA-A0201-250                  0.5                    32   \n",
       "35       tanh   HLA-A0201-250                  0.5                    32   \n",
       "42       tanh   HLA-A0201-250                  0.5                     4   \n",
       "36       tanh   HLA-A0201-250                  0.5                     4   \n",
       "39       tanh   HLA-A0201-250                  0.5                     4   \n",
       "45       tanh   HLA-A0201-250                  0.5                     4   \n",
       "44       tanh   HLA-A0201-250                  0.5                     4   \n",
       "46       tanh   HLA-A0201-250                  0.5                     4   \n",
       "19       tanh    HLA-A0201-10                  0.5                    32   \n",
       "29       tanh    HLA-A0201-10                  0.5                    32   \n",
       "23       tanh    HLA-A0201-10                  0.5                    32   \n",
       "20       tanh    HLA-A0201-10                  0.5                    32   \n",
       "24       tanh    HLA-A0201-10                  0.5                    32   \n",
       "25       tanh    HLA-A0201-10                  0.5                     4   \n",
       "16       tanh    HLA-A0201-10                  0.5                     4   \n",
       "21       tanh    HLA-A0201-10                  0.5                     4   \n",
       "27       tanh    HLA-A0201-10                  0.5                     4   \n",
       "17       tanh    HLA-A0201-10                  0.5                    32   \n",
       "22       tanh    HLA-A0201-10                  0.5                     4   \n",
       "26       tanh    HLA-A0201-10                  0.5                    32   \n",
       "31       tanh    HLA-A0201-10                  0.5                     4   \n",
       "30       tanh    HLA-A0201-10                  0.5                    32   \n",
       "28       tanh    HLA-A0201-10                  0.5                     4   \n",
       "18       tanh    HLA-A0201-10                  0.5                     4   \n",
       "\n",
       "     fit_time  fraction_negative impute layer_sizes  \\\n",
       "14  13.426505                0.1   True        [64]   \n",
       "5   12.238065                0.1  False        [64]   \n",
       "6   15.937529                0.1   True       [256]   \n",
       "9   10.549995                0.1  False         [4]   \n",
       "13  14.192804                0.1  False       [256]   \n",
       "2   11.623836                0.1   True         [4]   \n",
       "7   13.778689                0.1  False       [128]   \n",
       "3   10.565188                0.1   True       [128]   \n",
       "8   11.624597                0.1  False         [4]   \n",
       "11  13.419750                0.1  False       [256]   \n",
       "4   12.906013                0.1   True       [128]   \n",
       "0    9.648340                0.1   True        [64]   \n",
       "12  14.581950                0.1   True       [256]   \n",
       "15  12.843067                0.1   True         [4]   \n",
       "1    8.698821                0.1  False        [64]   \n",
       "10   8.868767                0.1  False       [128]   \n",
       "41   3.915362                0.1   True       [128]   \n",
       "32   5.799231                0.1   True       [256]   \n",
       "34   3.958769                0.1  False       [128]   \n",
       "40   4.495596                0.1   True         [4]   \n",
       "37   3.002649                0.1  False        [64]   \n",
       "33   4.275779                0.1  False       [256]   \n",
       "38   4.928189                0.1   True       [256]   \n",
       "47   5.147194                0.1   True       [128]   \n",
       "43   3.453106                0.1  False         [4]   \n",
       "35   5.516232                0.1   True        [64]   \n",
       "42   4.009234                0.1  False         [4]   \n",
       "36   3.019351                0.1  False        [64]   \n",
       "39   3.884387                0.1   True        [64]   \n",
       "45   3.678478                0.1  False       [128]   \n",
       "44   3.473869                0.1  False       [256]   \n",
       "46   5.252091                0.1   True         [4]   \n",
       "19   2.368640                0.1   True         [4]   \n",
       "29   2.609647                0.1   True       [256]   \n",
       "23   2.593981                0.1   True        [64]   \n",
       "20   2.303217                0.1   True       [128]   \n",
       "24   1.378262                0.1  False       [128]   \n",
       "25   2.395534                0.1   True         [4]   \n",
       "16   2.333883                0.1   True        [64]   \n",
       "21   2.406448                0.1   True       [128]   \n",
       "27   2.581795                0.1   True       [256]   \n",
       "17   1.392699                0.1  False       [256]   \n",
       "22   1.511511                0.1  False         [4]   \n",
       "26   1.415610                0.1  False        [64]   \n",
       "31   1.369998                0.1  False       [256]   \n",
       "30   1.365929                0.1  False         [4]   \n",
       "28   1.506067                0.1  False       [128]   \n",
       "18   1.354459                0.1  False        [64]   \n",
       "\n",
       "                                         model_params    pretrain_decay  \\\n",
       "14  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "5   {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "6   {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "9   {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "13  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "2   {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "7   {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "3   {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "8   {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "11  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "4   {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "0   {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "12  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "15  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "1   {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "10  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "41  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "32  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "34  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "40  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "37  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "33  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "38  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "47  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "43  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "35  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "42  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "36  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "39  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "45  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "44  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "46  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "19  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "29  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "23  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "20  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "24  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "25  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "16  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "21  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "27  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "17  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "22  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "26  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "31  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "30  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "28  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "18  {u'pretrain_decay': u'1 / (1+epoch)**2', u'imp...  1 / (1+epoch)**2   \n",
       "\n",
       "    test_auc   test_f1  test_size  test_tau  train_auc  train_f1  train_size  \\\n",
       "14  0.944920  0.792123       6532  0.578521   0.958178  0.846645        1000   \n",
       "5   0.944612  0.789052       6532  0.577878   0.957548  0.845161        1000   \n",
       "6   0.944024  0.774943       6532  0.578387   0.963812  0.844517        1000   \n",
       "9   0.943817  0.774866       6532  0.574753   0.954256  0.812500        1000   \n",
       "13  0.943776  0.800000       6532  0.575957   0.961420  0.860720        1000   \n",
       "2   0.943569  0.792068       6532  0.573797   0.953553  0.832000        1000   \n",
       "7   0.943261  0.788530       6532  0.574798   0.959685  0.863492        1000   \n",
       "3   0.943004  0.777246       6532  0.574835   0.962954  0.857143        1000   \n",
       "8   0.924926  0.661178       6532  0.550081   0.927693  0.708861        1000   \n",
       "11  0.924763  0.677850       6532  0.551431   0.926912  0.726950        1000   \n",
       "4   0.924209  0.713571       6532  0.549220   0.926638  0.752941        1000   \n",
       "0   0.923279  0.685073       6532  0.545595   0.922228  0.729167        1000   \n",
       "12  0.922751  0.684932       6532  0.544787   0.922721  0.735552        1000   \n",
       "15  0.922142  0.668957       6532  0.543873   0.923145  0.720848        1000   \n",
       "1   0.921942  0.709810       6532  0.546140   0.926168  0.770000        1000   \n",
       "10  0.921489  0.708394       6532  0.545831   0.924899  0.765391        1000   \n",
       "41  0.895955  0.558736       8482  0.517298   0.898150  0.545455         250   \n",
       "32  0.895112  0.677324       8482  0.515190   0.953719  0.775194         250   \n",
       "34  0.895087  0.659772       8482  0.515716   0.948301  0.750000         250   \n",
       "40  0.894884  0.654356       8482  0.515058   0.942651  0.755906         250   \n",
       "37  0.893989  0.658652       8482  0.514013   0.943658  0.766917         250   \n",
       "33  0.893219  0.660043       8482  0.514081   0.949694  0.763359         250   \n",
       "38  0.893073  0.550761       8482  0.512360   0.902097  0.545455         250   \n",
       "47  0.892656  0.666525       8482  0.511780   0.950159  0.778626         250   \n",
       "43  0.891843  0.640374       8482  0.511244   0.933364  0.725806         250   \n",
       "35  0.889847  0.666104       8482  0.507754   0.947682  0.763359         250   \n",
       "42  0.885435  0.535777       8482  0.503301   0.892888  0.523364         250   \n",
       "36  0.884448  0.545229       8482  0.497892   0.892114  0.571429         250   \n",
       "39  0.883239  0.541594       8482  0.498539   0.893352  0.589286         250   \n",
       "45  0.879062  0.498576       8482  0.491699   0.896989  0.545455         250   \n",
       "44  0.878753  0.495090       8482  0.493345   0.885226  0.576577         250   \n",
       "46  0.875744  0.472901       8482  0.485739   0.885535  0.537037         250   \n",
       "19  0.788338  0.039888       9492  0.364708   1.000000  0.800000          10   \n",
       "29  0.778861  0.015808       9492  0.353579   1.000000  1.000000          10   \n",
       "23  0.766717  0.016430       9492  0.339995   1.000000  1.000000          10   \n",
       "20  0.720564  0.014539       9492  0.281390   1.000000  1.000000          10   \n",
       "24  0.648763  0.020038       9492  0.186176   1.000000  1.000000          10   \n",
       "25  0.647545  0.016970       9492  0.191832   1.000000  0.500000          10   \n",
       "16  0.636350  0.013253       9492  0.173754   1.000000  0.800000          10   \n",
       "21  0.631081  0.047196       9492  0.175516   1.000000  0.800000          10   \n",
       "27  0.629064  0.039567       9492  0.165813   1.000000  0.500000          10   \n",
       "17  0.607756  0.006349       9492  0.145288   1.000000  1.000000          10   \n",
       "22  0.587999  0.178299       9492  0.095901   1.000000  0.500000          10   \n",
       "26  0.584683  0.025990       9492  0.111075   1.000000  1.000000          10   \n",
       "31  0.537523  0.035987       9492  0.043781   1.000000  0.800000          10   \n",
       "30  0.522576  0.066493       9492  0.028709   1.000000  0.800000          10   \n",
       "28  0.518687  0.035747       9492  0.024902   1.000000  0.500000          10   \n",
       "18  0.502695  0.038531       9492  0.004088   1.000000  0.500000          10   \n",
       "\n",
       "    train_tau  layer0_size                                       model_string  \n",
       "14   0.649511           64  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "5    0.648197           64  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "6    0.667076          256  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "9    0.640486            4  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "13   0.662286          256  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "2    0.639674            4  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "7    0.654281          128  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "3    0.663518          128  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "8    0.595068            4  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "11   0.593954          256  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "4    0.590873          128  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "0    0.585888           64  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "12   0.587438          256  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "15   0.586308            4  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "1    0.588279           64  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "10   0.590000          128  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "41   0.589767          128  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "32   0.683423          256  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "34   0.659014          128  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "40   0.650333            4  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "37   0.655685           64  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "33   0.663060          256  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "38   0.587157          256  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "47   0.667041          128  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "43   0.649420            4  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "35   0.665997           64  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "42   0.557592            4  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "36   0.568883           64  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "39   0.565685           64  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "45   0.576649          128  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "44   0.569862          256  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "46   0.545387            4  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "19   0.988826            4  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "29   0.943880          256  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "23   0.943880           64  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "20   0.988826          128  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "24   0.943880          128  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "25   0.898933            4  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "16   0.943880           64  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "21   0.943880          128  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "27   0.898933          256  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "17   0.943880          256  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "22   0.764093            4  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "26   0.943880           64  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "31   0.898933          256  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "30   0.943880            4  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "28   0.943880          128  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  \n",
       "18   0.674200           64  {'pretrain_decay': '1 / (1+epoch)**2', 'impute...  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df.sort(\"test_auc\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_df[\"combined\"] = cv_df.test_auc + cv_df.test_f1 + cv_df.test_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_df_str = cv_df.copy()\n",
    "print(cv_df_str.columns)\n",
    "del cv_df_str['model_params']\n",
    "del cv_df_str['fit_time']\n",
    "\n",
    "for col in [\"layer_sizes\"]:\n",
    "    cv_df_str[col] = [str(x) for x in cv_df_str[col]]\n",
    "summary = cv_df_str.groupby(list(cv_df_str.columns[:6])).mean() #.reset_index()\n",
    "summary.sort(\"combined\", ascending=False, inplace=True)\n",
    "summary.to_csv(\"../data/cv_hla0201_summary_128.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_df.sort(\"combined\", ascending=False, inplace=True)\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv_df = pandas.DataFrame(cv_df)\n",
    "cv_df[\"layer0_size\"] = [x[0] for x in cv_df.layer_sizes]\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv_df.to_csv(\"cv5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group_columns = [\"allele\", \"allele_size\", \"impute\"]\n",
    "group_columns.extend(models_params_explored)\n",
    "group_columns.append(\"layer0_size\")\n",
    "group_columns.remove(\"layer_sizes\")\n",
    "print(mean_with_std(cv_df.groupby(group_columns).test_auc)) #.sort(inplace=False, ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_by(score):\n",
    "    means = cv_df.groupby(group_columns)[score].mean().reset_index()\n",
    "    max_rows = []\n",
    "    for allele in means.allele.unique():\n",
    "        max_rows.append(means.ix[means.allele == allele][score].argmax())\n",
    "    return means.ix[max_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_by('test_auc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_by('test_tau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_by('test_f1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
